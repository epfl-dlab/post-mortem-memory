---
title: "Post-mortem memory of public figures in news and social media"
author: "Robert West, Jure Leskovec, Christopher Potts"
date: ""
output:
  html_document: default
  pdf_document:
    keep_tex: true
---

```{r include=FALSE}
# Set this to FALSE if you don't want to save plots to PDFs.
SAVE_PLOTS <- FALSE

# Begin of nasty trick from here: https://stackoverflow.com/a/46686678
SUPP_INFO <- FALSE
INCLUDE_REGRESSION_IN_SUPP_INFO <- TRUE
```
\newif\ifshowall
\showall`r ifelse(!SUPP_INFO, 'true', 'false')`
```{r include=FALSE}
# End of nasty trick.
# To exclude a text from the Supplemental Information (SI), wrap it in \ifshowall, \fi:
#   \ifshowall
#   This is not the SI
#   \fi
# Wrapped text is omitted from SI (LaTeX), but is shown in reproducibility script (HTML).

# To hide warnings and messages: warning=FALSE, message=FALSE
# To hide code but show output: echo=FALSE
# To hide output but show code: results="hide"
# To hide code and output: include=FALSE
```

```{r include=FALSE}
#library(knitr); knit_exit()
```

```{r load data and functions, echo=!SUPP_INFO, results="hide", message=FALSE}
library(Matrix)
library(cluster)
library(lattice)
library(plotrix)
library(gridExtra)
library(KernSmooth)
library(boot)
library(parallel)
library(texreg)
library(nnet)
library(xtable)
library(lpSolve)
library(knitr)

.default_par <- par(no.readonly=TRUE)

BASEDIR <- sprintf('%s/github/post-mortem-memory', Sys.getenv('HOME'))
DATADIR <- sprintf('%s/data/', BASEDIR)
PLOTDIR <- sprintf('%s/plots/', BASEDIR)
TABLEDIR <- sprintf('%s/tables/', BASEDIR)

fignum <- 0
tabnum <- 0
figcap <- function(increment) {
  if (increment) fignum <<- fignum + 1
  sprintf('**%sFigure %s:** ', if (SUPP_INFO) 'Supplementary ' else '', fignum)
}
tabcap <- function(increment) {
  if (increment) tabnum <<- tabnum + 1
  sprintf('**%sTable %s:** ', if (SUPP_INFO) 'Supplementary ' else '', tabnum)
}

vertskip <- '$$\\\\[4mm]$$'

# We aggregate chunk_size days; a value of 7 means we aggregate by weeks.
CHUNK_SIZE <- 1

# The number of time units ('chunks') before and after death.
N <- 360

COL <- list(NEWS=rgb(.9,.6,0), TWITTER=rgb(0,.45,.7))
COL_LIGHT <- list(NEWS=rgb(.9,.6,0,.3), TWITTER=rgb(0,.45,.7,.3))
COL_XLIGHT <- list(NEWS=rgb(.9,.6,0,.03), TWITTER=rgb(0,.45,.7,.03))
COL_LIGHTBLUE <- "#56B4E9"
COL_YELLOW <- "#F0E442"
COL_DARKBLUE <- "#0072B2"
COL_RED <- "#D55E00"
COL_MAGENTA <- "#CC79A7"
COL_GRAY <- "#999999"
COL_ORANGE <- "#E69F00"
COL_GREEN <- "#009E73"

ANGLO_COUNTRY_REGEX <- 'United States|United Kingdom|Canada|England|Australia|Scotland|Wales|Ireland|New Zealand|South Africa'

type_groups <- c('art', 'sports', 'leadership', 'known for death', 'general fame', 'academia/engineering')

split_at <- function(str, sep) strsplit(str, sep)[[1]]

fancy_mid <- function(mid) sub('<http://rdf.freebase.com/ns/m.(.*)>', '/m/\\1', mid)
long_mid <- function(mid) sub('/m/(.*)', '<http://rdf.freebase.com/ns/m.\\1>', mid)

fancy_wiki <- function(wiki) sub('<http://en.wikipedia.org/wiki/(.*)>', '\\1', wiki)
long_wiki <- function(wiki) sprintf('<http://en.wikipedia.org/wiki/%s>', wiki)

FANCY_CURVE_CHAR_NAMES <- c('Pre-mortem mean', 'Short-term boost', 'Long-term boost', 'Halving time')
names(FANCY_CURVE_CHAR_NAMES) <- c('mean_before', 'peak_mean_boost', 'perm_boost', 'time_till_half')

# A list of all days.
MIN_DATE <- as.Date("2008-08-01")
MAX_DATE <- as.Date("2014-09-30")
DAYS <- as.character(seq(MIN_DATE, MAX_DATE, by="+1 day"))
MAX_ABS_RELDATE <- as.numeric(MAX_DATE - MIN_DATE)

# Our Twitter data set is reasonably large only from 2009-06-11 on.
TWITTER_START_DATE <- as.Date("2009-06-11")

# We build a matrix x that has a row per person, and a column per day since death. There are
# nreldates = 2*(MAX_DATE-MIN_DATE)+1 such possible dates: the extreme cases are someone dying on
# MIN_DATE or MAX_DATE, so the relDates are [0 : MAX_DATE-MIN_DATE] in the former case, and
# [MIN_DATE-MAX_DATE : 0] in the latter case, for a total range of [MIN_DATE-MAX_DATE :
# MAX_DATE-MIN_DATE]; there are 2*(MAX_DATE-MIN_DATE)+1 values in this range.
# Values of relDate can be negative, zero, or positive. However, since R indices must be
# positive, we must translate relDates to get indices; the translation is as such:
# idx = relDate + MAX_DATE - MIN_DATE + 1
NRELDATES <- 2 * as.numeric(MAX_DATE - MIN_DATE) + 1

# The days on which our Spinn3r client must have been broken (there are still several tens of
# thousands of articles with dates from these days, but they were crawled on days outside of this
# window).
EMPTY_DAYS <- c(
  "2009-05-19", "2009-07-15", "2010-04-05", "2010-04-20", "2010-04-21", "2010-04-22", "2010-04-23", "2010-04-24",
  "2010-04-25", "2010-04-26", "2010-04-27", "2010-04-28", "2010-04-29", "2010-04-30", "2010-05-01", "2010-05-02", 
  "2010-05-03", "2010-05-04", "2010-05-05", "2010-05-06", "2010-05-07", "2010-05-08", "2010-05-09", "2010-05-10",
  "2010-05-11", "2010-05-12", "2010-05-13", "2010-05-16", "2010-05-17", "2010-05-18", "2010-05-19", "2010-05-20",
  "2010-05-21", "2010-05-22", "2010-05-23", "2010-05-24", "2010-05-25", "2010-05-26", "2010-05-27", "2010-05-28",
  "2010-05-29", "2010-05-30", "2010-05-31", "2010-06-01", "2010-06-02", "2010-06-03", "2010-06-04", "2010-06-05",
  "2010-06-06", "2010-06-07", "2010-06-08", "2010-06-09", "2010-06-10", "2010-06-11", "2010-06-12", "2010-06-13",
  "2010-06-14", "2010-06-15", "2010-06-16", "2010-06-17", "2010-06-18", "2010-06-19", "2010-06-20", "2010-06-21",
  "2010-06-22", "2010-06-23", "2010-06-24", "2010-06-25", "2010-06-26", "2010-06-27", "2010-06-28", "2010-06-29",
  "2010-06-30", "2010-07-01", "2010-07-02", "2010-07-03", "2010-07-04", "2010-07-05", "2010-07-06", "2010-07-07",
  "2010-07-08", "2010-07-09", "2010-07-10", "2010-07-11", "2010-07-12", "2010-07-13", "2011-12-03")

# The dates of death.
death_dates_tbl <- read.table(pipe(sprintf('gunzip -c %s/date_of_death.nt', DATADIR)),
                              comment.char='', sep='\t', quote='',
                              col.names=c('mid', '_rel', 'date', '_period'))[,c('mid', 'date')]
death_dates_tbl <- death_dates_tbl[grepl('#date>$', death_dates_tbl$date),]
death_dates_tbl$date <- substring(death_dates_tbl$date, 2, 11)
death_dates <- death_dates_tbl$date
names(death_dates) <- death_dates_tbl$mid

# Our Twitter data set is reasonably large only from 2009-06-11 on.
# Get the mids of the people that died between then and MAX_DATE (Sept. 2014).
died_in_window <- names(death_dates[as.Date(death_dates) >= TWITTER_START_DATE
                                    & as.Date(death_dates) <= MAX_DATE])

# A mapping from mids to Wikipedia names.
wiki_tbl <- read.table(pipe(sprintf('gunzip -c %s/names.DEAD.UNAMBIGUOUS.tsv.gz', DATADIR)),
                       comment.char='', sep='\t', quote='',
                       col.names=c('mid', 'names', 'aliases', 'curid', 'wiki'),
                       stringsAsFactors=FALSE)
wiki_tbl$wiki <- sapply(wiki_tbl$wiki, function(x) split_at(x, '\\|')[1])
mid_to_wiki <- wiki_tbl$wiki
names(mid_to_wiki) <- wiki_tbl$mid
mid_to_wiki <- mid_to_wiki[!is.na(mid_to_wiki)]

# A mapping from mids to names.
wiki_to_mid <- names(mid_to_wiki)
names(wiki_to_mid) <- mid_to_wiki

# Properties of dead people.
props <- read.table(pipe(sprintf('gunzip -c %s/dead_people_properties.tsv.gz', DATADIR)),
                    comment.char='', sep='\t', quote='', header=TRUE, fill=TRUE,
                    col.names=c('person', 'cause_of_death', 'date_of_death', 'place_of_death', 'date_of_burial',
                                'place_of_burial', 'date_of_cremation', 'place_of_cremation', 'date_of_birth',
                                'place_of_birth', 'nationality', 'profession', 'religion', 'ethnicity',
                                'notable_types', 'gender'))
props$mid <- sub('(<.*>).*', '\\1', props$person)
props$gender <- as.factor(sub('<.*>(.*)', '\\1', props$gender))
death_years <- sub('"(....).*', '\\1', props$date_of_death)
birth_years <- sub('"(....).*', '\\1', props$date_of_birth)
death_years[!grepl("....", death_years)] <- NA
birth_years[!grepl("....", birth_years)] <- NA
props$age <- as.numeric(death_years) - as.numeric(birth_years)
rownames(props) <- props$mid

# Load the taxonomies.
tax_causes <- read.table(sprintf('%s/taxonomy_causes_of_death.tsv', DATADIR),
                         header=TRUE, sep='\t', comment.char='', quote='')
rownames(tax_causes) <- paste(tax_causes$mid, tax_causes$cause_of_death, sep='')
tax_types <- read.table(sprintf('%s/taxonomy_notable_types.tsv', DATADIR),
                        header=TRUE, sep='\t', comment.char='', quote='')
rownames(tax_types) <- paste(tax_types$mid, tax_types$notable_type, sep='')

# Some causes of death are missing from Janice's table. Add them manually.
natural_manual <- "Viral pneumonia|Smallpox|Dementia with Lewy bodies|Heart valve disease|Creutzfeldt–Jakob disease|T-Cell Lymphoma|Adrenocortical carcinoma|Huntington's disease|Congenital heart defect|Squamous-cell carcinoma|Atypical teratoid rhabdoid tumor|Alveolar rhabdomyosarcoma|Appendix cancer|Pyelonephritis|Polymyalgia rheumatica|Polycythemia|Leiomyosarcoma|Astrocytoma"
unnatural_manual <- "Smoke inhalation|Racing Accident|Lightning|Casualty of war|Cocaine overdose|Poisoning|Shootout|Murder–suicide|Accidental drug overdose|Blast injury"

# Create the regexes for (un)natural deaths.
natural_regex <- sprintf(">(%s|%s)($|\\|)",
                         paste(tax_causes$cause.of.death[tax_causes$level1=='natural'],
                               collapse='|'), natural_manual)
unnatural_regex <- sprintf(">(%s|%s)($|\\|)",
                           paste(tax_causes$cause.of.death[tax_causes$level1=='unnatural'],
                                 collapse='|'), unnatural_manual)

# -1 = unnatural, 0 = unknown/conflicting, 1 = natural.
get_cause_of_death_map <- function() {
  natural_death_mids <- props$mid[which(grepl(natural_regex, props$cause_of_death))]
  unnatural_death_mids <- props$mid[which(grepl(unnatural_regex, props$cause_of_death))]
  map <- (props$mid %in% natural_death_mids) - (props$mid %in% unnatural_death_mids)
  # If a person has a natural and an unnatural cause, we want to list them under unnatural death.
  map[props$mid %in% natural_death_mids & props$mid %in% unnatural_death_mids] <- -1
  names(map) <- props$mid
  return(map)
}

# e.g., <http://rdf.freebase.com/ns/m.025698c>Baseball Player --> <http://rdf.freebase.com/ns/m.025698c>
strip_plaintext_name_from_mid <- function(mid_with_name) {
  sub('(<.*>).*', '\\1', mid_with_name)
}

# tax must be either tax_types or tax_causes;
# data must be a vector with taxonomy entries as values and mids as names.
select_from_tax <- function(level, value, tax, data) {
  col <- sprintf('level%d', level)
  ok_values <- rownames(tax)[tax[,col] == value]
  names(data)[data %in% ok_values]
}

get_num_art <- function(medium) {
  if (medium != 'NEWS' && medium != 'TWITTER') {
    stop('Medium must be \'NEWS\' or \'TWITTER\'')
  }
  num_art_tbl <- read.table(sprintf('%s/num_articles_per_day_%s.tsv', DATADIR, medium),
                            col.names=c('date', 'num'))
  num_art <- num_art_tbl$num
  names(num_art) <- num_art_tbl$date
  num_art <- num_art[names(num_art) >= as.character(MIN_DATE) & names(num_art) <= as.character(MAX_DATE)]
  num_art[setdiff(DAYS, names(num_art))] <- 0
  num_art <- num_art[order(names(num_art))]
  num_art[EMPTY_DAYS] <- NA
  return(num_art)
}

get_mention_freq_table <- function(medium) {
  if (medium != 'NEWS' && medium != 'TWITTER') {
    stop('Medium must be \'NEWS\' or \'TWITTER\'')
  }
  file <- sprintf('%s/RData/dead_people_mentions_%s.RData', DATADIR, medium)
  if (!file.exists(file)) {
    data <- read.table(pipe(sprintf('gunzip -c %s/num_dead_mentions_per_day_%s.tsv.gz', DATADIR, medium)),
                       comment.char='', sep='\t', quote='',
                       col.names=c('mid', 'date', 'num_per_doc', 'num_doc'),
                       colClasses=c('character', 'character', 'numeric', 'numeric'))
    # Add a column having the number of days since death.
    data$rel_date <- as.numeric(as.Date(data$date) - as.Date(death_dates[data$mid]))
    save(data, file=file)
  } else {
    load(file)
  }
  return(data)
}

get_rel_date_matrix <- function(medium, data, num_art, chunk_size) {
  if (medium == 'NEWS') {
    min_num_per_doc <- 2
  } else if (medium == 'TWITTER') {
    min_num_per_doc <- 1
  } else {
    stop('Medium must be \'NEWS\' or \'TWITTER\'')
  }
  chunks <- floor(((1:NRELDATES)-(NRELDATES+1)/2)/chunk_size)
  sum.na.rm <- function(x) sum(x, na.rm=TRUE)
  idx <- which(data$num_per_doc >= min_num_per_doc)
  file <- sprintf('%s/RData/num_mentions_per_rel_date_%s_min_num_per_doc=%s_chunk_size=%s.RData',
                  DATADIR, medium, min_num_per_doc, chunk_size)
  if (!file.exists(file)) {
    x <- do.call(rbind, mclapply(split(data[idx,], data$mid[idx]), function(l) {
      dod <- as.Date(death_dates[l$mid[1]])
      # Aggregate the counts of docs containing 1, 2, >=3 mentions.
      l <- data.frame(t(simplify2array(by(l, l$date, function(X) c(
        as.numeric(as.character(X$rel_date[1])),
        as.numeric(as.character(sum(X$num_doc))))))))
      colnames(l) <- c('rel_date', 'num_doc')
      l$date <- rownames(l)
      # Initialize y, the count vector for the current mid.
      # y has 2*(MAX_DATE-MIN_DATE)+1 entries, of which only MAX_DATE-MIN_DATE+1 are well-defined (as per
      # our discussion of NRELDATES above).
      # For the MAX_DATE-MIN_DATE days for which we have no data, the values are set to NA.
      # We also set to NA the values for the days that have no Spinn3r data (primarily the 2010 hole).
      # For the remaining MAX_DATE-MIN_DATE+1 days, we initialize values to 0.
      offset <- as.numeric(MAX_DATE-MIN_DATE) + 1
      y <- rep(NA, NRELDATES)
      y[as.numeric(as.Date(DAYS)-dod) + offset] <- 0
      y[l$rel_date + offset] <- l$num_doc
      y[as.numeric(as.Date(EMPTY_DAYS)-dod) + offset] <- NA
      # n: the number of docs per day aligned in terms of relative dates.
      n <- rep(0, NRELDATES)
      n[as.numeric(as.Date(DAYS)-dod) + offset] <- num_art
      # Sum within chunks.
      s <- tapply(y, chunks, sum.na.rm) / tapply(n, chunks, sum.na.rm)
      # Set to NA the days before TWITTER_START_DATE in the case of Twitter.
      if (medium == 'TWITTER') {
        s[as.numeric(as.Date(DAYS[as.Date(DAYS) < TWITTER_START_DATE])-dod) + offset] <- NA
      }
      return(s)
    }, mc.cores=6))
    x <- as.matrix(x)
    save(x, file=file)
  } else {
    load(file)
  }
  return(x)
}

filter_people <- function(medium, x) {
  N_immed_after <- 100
  num_finite_before <- rowSums(is.finite(x[,colnames(x) %in% -N:-1]))
  num_finite_immed_after <- rowSums(is.finite(x[,colnames(x) %in% 0:(N_immed_after-1)]))
  num_active_before <- rowSums(x[,colnames(x) %in% -N:-1] > 0, na.rm=TRUE)
  mids <- names(which(
    # Keep only people whose boundaries aren't missing.
    is.finite(x[,colnames(x)==N]) & is.finite(x[,colnames(x)==-N])
    # Keep only people that have no missing data in the N_immed_after days immediately after death.
    & num_finite_immed_after == N_immed_after
    # Keep only people that were mentioned on at least 5 days before they died.
    & num_active_before >= 5
    # Keep only people that died after the point from which on we have a lot of Twitter data.
    & rownames(x) %in% died_in_window
    # Discard people with parentheses in their names because those names, although unique in
    # Wikipedia, are unlikely to ever be used in real prose.
    & !grepl('\\(', mid_to_wiki[rownames(x)])))
  return(mids)
}

# Apply Friedman's super smoother, hell yeah.
supersmooth <- function(y) {
  reldates <- as.numeric(names(y))
  suppressWarnings({
    smoothed_left <- supsmu(reldates[reldates<0], y[reldates<0])
    smoothed_right <- supsmu(reldates[reldates>=0], y[reldates>=0])
  })
  smoothed <- c(smoothed_left$y, smoothed_right$y)
  names(smoothed) <- c(smoothed_left$x, smoothed_right$x)
  return(smoothed)
}

normalize_and_smooth <- function(medium, x, num_art, mean_center=TRUE) {
  if (medium != 'NEWS' && medium != 'TWITTER') {
    stop('Medium must be \'NEWS\' or \'TWITTER\'')
  }
  # Smoothing term: we add 1 mention (as computed on the highest-volume day) to each day.
  eps <- 1 / max(num_art, na.rm=TRUE)
  file <- sprintf('%s/RData/clustering_input_%s%s.RData', DATADIR, medium, if (mean_center) '_MEANCENTERED' else '')
  if (!file.exists(file)) {
    # Select the subset of reldates; keep a month of padding, so smoothing is more robust at the
    # boundaries,
    X <- x[,colnames(x) %in% -(N+30):(N+30)]
    reldates <- as.numeric(colnames(X))
    # Add-eps smoothing.
    X <- X + eps
    # Normalize and smooth all time series.
    X <- t(apply(X, 1, function(y_unnorm) {
      # Log10-transform.
      y <- log10(y_unnorm)
      # Smooth.
      y <- supersmooth(y)
      # Subtract the pre-mortem mean. Now we have log10(y_unnorm/mean*(y_unnorm)), where mean* is the
      # exponential of the mean in log space (i.e., a more robust version of the mean).
      # In the mean computation we exclude the 30 days immediately before death, to mitigate the
      # impact of the final sick days for people whose death was foreseeable.
      if (mean_center) y <- y - mean(y[reldates %in% -N:-30], na.rm=TRUE)
      # Select the subset of reldates.
      y <- y[names(y) %in% -N:N]
      # Interpolate missing values; rule=2 means that at the boundaries the values at the boundaries are
      # imputed for missing values. (But since we require our time series not to end in a missing value,
      # this shouldn't happen in the post-mortem period.)
      y <- approx(names(y), y, -N:N, rule=2)$y
      return(y)
    }))
    colnames(X) <- -N:N
    save(X, file=file)
  } else {
    load(file)
  }
  return(X)
}

# Normalize the time series to [0,1] and measure the fraction of time it takes until half of the
# total post-mortem volume has been seen. The simplest version maps the min to 0, but this is quite
# sensitive to low outliers, so we also allow for versions where we first set to 0 all values
# less than the specified quantile.
time_till_half <- function(y, thresh_quantile=0) {
  # Start with the day after death, since the news might not be reporting yet on the day of.
  y <- y[as.numeric(names(y)) >= 1]
  y <- pmax(y, quantile(y, thresh_quantile))
  y <- (y - min(y)) / (max(y) - min(y))
  min(which(cumsum(y) >= 0.5*sum(y))) / length(y)
}

compute_curve_stats <- function(medium, x, X, num_art) {
  if (medium != 'NEWS' && medium != 'TWITTER') {
    stop('Medium must be \'NEWS\' or \'TWITTER\'')
  }

  file <- sprintf('%s/mention_freq_for_spreadsheet_%s.csv', DATADIR, medium)
  if (!file.exists(file)) {
    # Find the people that died (un)natural deaths.
    mids <- rownames(x)
    mid_nat <- intersect(mids, names(which(get_cause_of_death_map()==1)))
    mid_unnat <- intersect(mids, names(which(get_cause_of_death_map()==-1)))

    # The number of days we consider before and after death.
    subrange_before <- -N:-30
    subrange_peak <- 0:29
    subrange_after <- 30:N
    
    x <- x[,colnames(x) %in% -N:N]
    
    num_active <- rowSums(x > 0, na.rm=TRUE)
    num_active_before <- rowSums(x[,colnames(x) %in% -N:-1] > 0, na.rm=TRUE)
    num_active_after <- rowSums(x[,colnames(x) %in% 0:N] > 0, na.rm=TRUE)
    
    num_finite_before <- rowSums(is.finite(x[,colnames(x) %in% -N:-1]))
    num_finite_after <- rowSums(is.finite(x[,colnames(x) %in% 0:N]))
    
    stats <- data.frame(mid=NA,
                        name=NA,
                        image=NA,
                        num_active_days=NA,
                        active_fraction_before=NA,
                        active_fraction_after=NA,
                        max_smoothed_before=NA,
                        max_raw_before=NA,
                        mean_before=NA,
                        mean_after=NA,
                        mean_after_peak=NA,
                        peak_smoothed=NA,
                        peak_raw=NA,
                        peak_raw_day=NA,
                        time_till_half=NA,
                        time_till_half_thresh=NA,
                        natural_death=NA,
                        unnatural_death=NA,
                        gender=NA,
                        cause_of_death=NA,
                        age=NA,
                        notable_type=NA)[-1,]

    for (i in 1:nrow(X)) {
      if (i %% 100 == 0) print(i)
      mid <- rownames(X)[i]
      name <- fancy_wiki(mid_to_wiki[mid])
      img_name <- if (!is.na(name)) gsub('(%(25)?..)+', '+', name) else sprintf('NA_%s', gsub('.*/m\\.(.*)>', '\\1', mid))
      raw <- log10(x[mid,])
      smoothed <- X[mid,]
      
      mean_before <- mean(smoothed[names(smoothed) %in% subrange_before], na.rm=TRUE)
      peak_smoothed <- max(smoothed[names(smoothed) %in% subrange_peak])
      peak_raw <- max(raw[names(raw) %in% subrange_peak])
      # -1 such that 0 represents day of death.
      peak_raw_day <- which.max(raw[names(raw) %in% subrange_peak]) - 1
      
      stats[mid,] <- c(
        fancy_mid(mid),
        name,
        sprintf('=image("http://infolab.stanford.edu/~west1/death/mention_freq_curves/%s/%s.png")',
                medium, img_name),
        num_active[mid],
        num_active_before[mid]/num_finite_before[mid],
        num_active_after[mid]/num_finite_after[mid],
        max(smoothed[names(smoothed) %in% subrange_before], na.rm=TRUE),
        max(raw[names(raw) %in% subrange_before], na.rm=TRUE),
        mean_before,
        mean(smoothed[names(smoothed) %in% c(subrange_peak, subrange_after)]),
        mean(smoothed[names(smoothed) %in% subrange_after]),
        peak_smoothed,
        peak_raw,
        peak_raw_day,
        time_till_half(smoothed),
        time_till_half(smoothed, 0.25),
        if (mid %in% mid_nat) 1 else 0,
        if (mid %in% mid_unnat) 1 else 0,
        as.character(props[mid, 'gender']),
        as.character(props[mid, 'cause_of_death']),
        props[mid, 'age'],
        as.character(props[mid, 'notable_types'])
      )
    }

    # Write the data into a CSV file.
    write.table(stats, file, quote=FALSE, sep='\t', row.names=FALSE)
  } else {
    stats <- read.table(file, header=TRUE, sep='\t', comment.char='', quote='')
  }
  return(stats)
}

load_stats <- function(medium, x, X, num_art) {
  if (medium != 'NEWS' && medium != 'TWITTER') {
    stop('Medium must be \'NEWS\' or \'TWITTER\'')
  }
  stats <- compute_curve_stats(medium, x, X, num_art)
  rownames(stats) <- long_mid(stats$mid)
  # If the person has 0 mentions on the day of death, the log will be -Inf.
  # Replace those by the smallest finite number.
  stats$peak_raw[!is.finite(stats$peak_raw)] <- min(stats$peak_raw[is.finite(stats$peak_raw)])
  # Since the quantities are logarithmic (log10), taking the difference is equivalent to the log10
  # of the ratio.
  stats$peak_mean_boost <- stats$peak_raw - stats$mean_before
  stats$peak_max_boost <- stats$peak_raw - stats$max_raw_before
  stats$perm_boost <- stats$mean_after_peak - stats$mean_before
  stats$perm_boost_all <- stats$mean_after - stats$mean_before
  # Some rank transforms.
  r <- function(x) {
    n <- length(x)
    # Keys: indices in x; values: ranks.
    rank_map <- 1:n
    names(rank_map) <- order(x)
    rank_map[as.character(1:n)]
  }
  stats$mean_before_rank <- r(stats$mean_before)
  stats$mean_after_rank <- r(stats$mean_after)
  stats$peak_mean_boost_rank <- r(stats$peak_mean_boost)
  stats$perm_boost_rank <- r(stats$perm_boost)
  stats$perm_boost_all_rank <- r(stats$perm_boost_all)
  stats$time_till_half_rank <- r(stats$time_till_half)
  # Add a categorical death-type column.
  stats$death_type <- 'unknown'
  stats$death_type[stats$natural_death==1] <- 'natural'
  stats$death_type[stats$unnatural_death==1] <- 'unnatural'
  stats$death_type <- as.factor(stats$death_type)
  # Add age groups.
  stats$age_group <- floor(stats$age/10)*10
  # Add anglo flag.
  country_known <- props$mid[props$nationality!='']
  anglos <- props$mid[grepl(ANGLO_COUNTRY_REGEX, props$nationality)]
  stats$anglo <- 'unknown'
  stats$anglo[rownames(stats) %in% country_known] <- 'non_anglo'
  stats$anglo[rownames(stats) %in% anglos] <- 'anglo'
  stats$anglo <- as.factor(stats$anglo)
  # Notable types for all people.
  types <- tax_types[strip_plaintext_name_from_mid(props$notable_types), 'level1']
  names(types) <- props$mid
  stats$type_group <- types[rownames(stats)]
  return(stats)
}

# The stats of all people, rather than just the couple of thousands included in our study.
load_stats_all <- function() {
  stats_all <- props[props$mid %in% died_in_window, c('gender', 'age', 'cause_of_death', 'nationality', 'notable_types')]
  stats_all$death_type <- 'unknown'
  stats_all$death_type[rownames(stats_all) %in% names(which(get_cause_of_death_map()==1))] <- 'natural'
  stats_all$death_type[rownames(stats_all) %in% names(which(get_cause_of_death_map()==-1))] <- 'unnatural'
  stats_all$death_type <- as.factor(stats_all$death_type)
  country_known <- rownames(stats_all)[stats_all$nationality!='']
  anglos <- rownames(stats_all)[grepl(ANGLO_COUNTRY_REGEX, stats_all$nationality)]
  stats_all$anglo <- 'unknown'
  stats_all$anglo[rownames(stats_all) %in% country_known] <- 'non_anglo'
  stats_all$anglo[rownames(stats_all) %in% anglos] <- 'anglo'
  stats_all$anglo <- as.factor(stats_all$anglo)
  types <- tax_types[strip_plaintext_name_from_mid(stats_all$notable_types), 'level1']
  names(types) <- rownames(stats_all)
  stats_all$type_group <- types[rownames(stats_all)]
  return(stats_all)
}

make_bio_table <- function() {
  to_percentage <- function(x) sprintf('%.0f%%', x*100)
  summary_table_all <- NULL
  summary_table_filtered <- NULL
  summary_table_lm <- NULL
  
  ### Age
  
  summary_table_filtered['Age'] <- ''
  summary_table_all['Age'] <- ''
  summary_table_lm['Age'] <- ''
  
  summary_table_filtered['Age N/A'] <- to_percentage(mean(is.na(stats_N$age)))
  summary_table_filtered[c('1st quartile', 'Mean', 'Median', '3rd quartile')] <- sprintf('%.0f', summary(stats_N$age)[c(2,4,3,5)])
  summary_table_all['Age N/A'] <- to_percentage(mean(is.na(stats_all$age)))
  summary_table_all[c('1st quartile', 'Mean', 'Median', '3rd quartile')] <- sprintf('%.0f', summary(stats_all$age[stats_all$age <= 120])[c(2,4,3,5)])
  summary_table_lm['Age N/A'] <- to_percentage(mean(is.na(lmdata_N$age)))
  summary_table_lm[c('1st quartile', 'Mean', 'Median', '3rd quartile')] <- sprintf('%.0f', summary(lmdata_N$age)[c(2,4,3,5)])
  
  ### Gender
  
  summary_table_filtered['Gender'] <- ''
  summary_table_all['Gender'] <- ''
  summary_table_lm['Gender'] <- ''
  
  idx <- stats_N$gender %in% c('Female', 'Male')
  summary_table_filtered['Gender N/A'] <- to_percentage(mean(!idx))
  summary_table_filtered[c('Female', 'Male')] <- to_percentage((summary(stats_N$gender[idx]) / sum(summary(stats_N$gender[idx])))[c('Female', 'Male')])
  
  idx <- stats_all$gender %in% c('Female', 'Male')
  summary_table_all['Gender N/A'] <- to_percentage(mean(!idx))
  summary_table_all[c('Female', 'Male')] <- to_percentage((summary(stats_all$gender[idx]) / sum(summary(stats_all$gender[idx])))[c('Female', 'Male')])
  
  idx <- lmdata_N$gender %in% c('Female', 'Male')
  summary_table_lm['Gender N/A'] <- to_percentage(mean(!idx))
  summary_table_lm[c('Female', 'Male')] <- to_percentage((summary(lmdata_N$gender[idx]) / sum(summary(lmdata_N$gender[idx])))[c('Female', 'Male')])
  
  ### Manner of death
  
  summary_table_filtered['Manner of death'] <- ''
  summary_table_all['Manner of death'] <- ''
  summary_table_lm['Manner of death'] <- ''
  
  idx <- stats_N$death_type != 'unknown'
  summary_table_filtered['Manner of death N/A'] <- to_percentage(mean(!idx))
  summary_table_filtered[c('Natural', 'Unnatural')] <-
    to_percentage((summary(stats_N$death_type[idx]) / sum(summary(stats_N$death_type[idx])))[c('natural', 'unnatural')])
  
  idx <- stats_all$death_type != 'unknown'
  summary_table_all['Manner of death N/A'] <- to_percentage(mean(!idx))
  summary_table_all[c('Natural', 'Unnatural')] <-
    to_percentage((summary(stats_all$death_type[idx]) / sum(summary(stats_all$death_type[idx])))[c('natural', 'unnatural')])
  
  idx <- lmdata_N$death_type != 'unknown'
  summary_table_lm['Manner of death N/A'] <- to_percentage(mean(!idx))
  summary_table_lm[c('Natural', 'Unnatural')] <-
    to_percentage((summary(lmdata_N$death_type[idx]) / sum(summary(lmdata_N$death_type[idx])))[c('natural', 'unnatural')])
  
  ### Language
  
  summary_table_filtered['Language'] <- ''
  summary_table_all['Language'] <- ''
  summary_table_lm['Language'] <- ''
  
  idx <- stats_N$anglo != 'unknown'
  summary_table_filtered['Language N/A'] <- to_percentage(mean(!idx))
  summary_table_filtered[c('Anglophone', 'Non-anglophone')] <- to_percentage((summary(stats_N$anglo[idx]) / sum(summary(stats_N$anglo[idx])))[c('anglo', 'non_anglo')])
  
  idx <- stats_all$anglo != 'unknown'
  summary_table_all['Language N/A'] <- to_percentage(mean(!idx))
  summary_table_all[c('Anglophone', 'Non-anglophone')] <- to_percentage((summary(stats_all$anglo[idx]) / sum(summary(stats_all$anglo[idx])))[c('anglo', 'non_anglo')])
  
  idx <- lmdata_N$anglo != 'unknown'
  summary_table_lm['Language N/A'] <- to_percentage(mean(!idx))
  summary_table_lm[c('Anglophone', 'Non-anglophone')] <- to_percentage((summary(lmdata_N$anglo[idx]) / sum(summary(lmdata_N$anglo[idx])))[c('anglo', 'non_anglo')])
  
  ### Notable type
  
  summary_table_filtered['Notability type'] <- ''
  summary_table_all['Notability type'] <- ''
  summary_table_lm['Notability type'] <- ''
  
  idx <- !is.na(stats_N$type_group)
  summ <- summary(stats_N$type_group[idx])[type_groups]
  summary_table_filtered['Notability type N/A'] <- to_percentage(mean(!idx))
  summary_table_filtered[names(summ)] <- to_percentage(summ / sum(summ))
  
  idx <- !is.na(stats_all$type_group)
  summ <- summary(stats_all$type_group[idx])[type_groups]
  summary_table_all['Notability type N/A'] <- to_percentage(mean(!idx))
  summary_table_all[names(summ)] <- to_percentage(summ / sum(summ))
  
  idx <- !is.na(lmdata_N$type_group)
  summ <- summary(lmdata_N$type_group[idx])[type_groups]
  summary_table_lm['Notability type N/A'] <- to_percentage(mean(!idx))
  summary_table_lm[names(summ)] <- to_percentage(summ / sum(summ))
  
  summary_table_filtered['Count'] <- nrow(stats_N)
  summary_table_all['Count'] <- nrow(stats_all)
  summary_table_lm['Count'] <- nrow(lmdata_N)
  
  # Return a table with stats for both "all" and "filtered" (i.e., those included in the study).
  data.frame(All=summary_table_all, Included=summary_table_filtered, Regression=summary_table_lm)
}

plot_num_art <- function(num_art, medium) {
  par(mar=c(6,5,2,2))
  idx <- which(names(num_art)==TWITTER_START_DATE):which(names(num_art)==MAX_DATE)
  plot(num_art[idx], axes=FALSE, log='y', type='p', xlab='', ylab='Number of documents',
       col=COL[[medium]], main=medium, ylim=c(5e4,1e8))
  axis(2)
  days <- DAYS[idx]
  ticks <- which(grepl('-01$', days))
  axis(1, at=ticks, labels=rep('', length(ticks)), col.ticks='gray')
  ticks <- which(grepl('-01-01$', days))
  axis(1, at=ticks, labels=days[ticks], las=2)
  # par(.default_par)
}

plot_time_series_for_small_multiples <- function(name, medium, X, x, num_art,
                                                 xlab=TRUE, ylab=TRUE) {
  if (SAVE_PLOTS) pdf(sprintf('%s/mention_curve_%s_%s.pdf', PLOTDIR, name, medium), width=2, height=2,
                      pointsize=8, family='Helvetica', useDingbats=FALSE)
  par(mar=c(3,3,1,1))
  mid <- wiki_to_mid[sprintf('<http://en.wikipedia.org/wiki/%s>', name)]
  ymin <- log10(1 / max(num_art, na.rm=TRUE))
  raw <- log10(x[mid, colnames(x) %in% -100:360])
  smoothed <- X[mid, colnames(X) %in% -100:360]
  plot(-100:360, raw, col=COL_LIGHT[[medium]],
       bty='n', xlab='', xlim=c(-100,360), ylim=c(ymin,-1), ylab='',
       panel.first=abline(v=0, col='gray'))
  text(360, -1, sprintf('%s', gsub('_', ' ', name)), adj=c(1,1), cex=1)
  lines(names(smoothed), smoothed, col=COL[[medium]], lwd=2)
  if (xlab) mtext('Days since death', side=1, line=2)
  if (ylab) mtext('Fraction mentioning docs [log10]', side=2, line=2)
  if (SAVE_PLOTS) dev.off()
}

plot_max_mem_hist <- function(medium) {
  if (SAVE_PLOTS) cairo_pdf(sprintf('%s/max_mem_hist_%s.pdf', PLOTDIR, medium), width=3.4, height=2.4,
                            pointsize=8, family='Helvetica')
  par(mar=c(3,3,1,3))
  if (medium == 'NEWS') {
    x <- x_N
  } else if (medium == 'TWITTER') {
    x <- x_T
  } else {
    stop("medium must be NEWS or TWITTER!")
  }
  t_max <- 100
  max_days <- apply(x[,colnames(x) %in% 0:t_max], 1, function(x) which.max(x) - 1)
  h <- hist(max_days, breaks=0:(t_max+1), right=FALSE, include.lowest=FALSE, xlab='', ylab='',
            main=NULL, axes=FALSE, col=COL[[medium]], border=NA, xlim=c(0,t_max+1), ylim=c(0,800))
  cum <- cumsum(h$counts)/sum(h$counts)
  ticks <- seq(0, t_max, 10)
  axis(1, at=ticks+0.5, labels=ticks, las=2)
  mtext('Days since death', side=1, line=2)
  axis(2, col=COL[[medium]], col.axis=COL[[medium]])
  mtext('Number of people', side=2, line=2, col=COL[[medium]])
  par(new=TRUE)
  plot((0:t_max)+0.5, cum, type='p', xlab='', ylab='', xlim=c(0,t_max+1), ylim=c(0,1), axes=FALSE,
       panel.first=abline(h=cum[30], v=29.5, col='gray', lty=2))
  axis(4)
  mtext('Cumulative rel. frequency', side=4, line=2)
  text(29.5, cum[30], labels=sprintf('%.1f%% had maximum mention\n frequency by day 29', 100*cum[30]),
       adj=c(-0.1,1.5), col='gray')
  if (SAVE_PLOTS) dev.off()
  return(list(hist=h, max_days=max_days))
}

make_df_for_fit <- function(medium) {
  if (medium == 'NEWS') {
    x <- x_N
  } else if (medium == 'TWITTER') {
    x <- x_T
  } else {
    stop("medium must be NEWS or TWITTER!")
  }
  
  t <- 1:400
  eps <- min(x[x>0], na.rm=TRUE)
  y <- 10^colMeans(log10(x[,colnames(x) %in% t] + eps), na.rm=TRUE)
  df <- data.frame(y=y, t=t)
  return(df)
}

biexp_fit <- function(medium) {
  df <- make_df_for_fit(medium)
  
  # First approximation: r = 0.
  # log(y) ~ log(N*exp(-p*t)) = log(N)-p*t
  model0 <- lm(log(y) ~ t, data=df)
  N0 <- exp(coef(model0)[1])
  p0 <- -coef(model0)[2]
  
  # Second approximation: q = 0.
  model1 <- nls(log(y) ~ log( N/(p+r) * (p*exp(-(p+r)*t) + r) ),
                start=list(p=p0, N=N0, r=0),
                data=df, control=list(maxiter=1e8), algorithm='port')
  N1 <- coef(model1)['N']
  p1 <- coef(model1)['p']
  r1 <- coef(model1)['r']
  
  # Full model.
  model <- nls(log(y) ~ log( N/(p-q+r) * ((p-q)*exp(-(p+r)*t) + r*exp(-q*t)) ),
               start=list(p=p1, N=N1, r=r1, q=0),
               data=df, control=list(maxiter=1e8), algorithm='port', lower=c(q=0))
  return(model)
}

exp_fit <- function(medium) {
  df <- make_df_for_fit(medium)
  # log(y) ~ log(a*exp(-b*t)) = log(a)-b*t
  model <- lm(log(y) ~ t, data=df)
  return(model)
}

exp_power_fit <- function(medium) {
  df <- make_df_for_fit(medium)
  # Start with a good guess.
  # log(y) ~ log(a*exp(b*t^(-1))) = log(a)+b*t^(-1)
  c0 <- -1
  model0 <- lm(log(y) ~ I(t^c0), data=df)
  log_a0 <- coef(model0)[1]
  b0 <- coef(model0)[2]
  model <- nls(log(y) ~ log_a + b*t^c,
               start=list(log_a=log_a0, b=b0, c=c0),
               data=df, control=list(maxiter=1e8), algorithm='port')
  
  return(model)
}

lognorm_fit <- function(medium) {
  df <- make_df_for_fit(medium)
  # log(y) ~ a + b*log(t) + c*log(t)^2
  model <- lm(log(y) ~ log(t) + I(log(t)^2), data=df)
  return(model)
}

lognorm_constrained_fit <- function(medium) {
  df <- make_df_for_fit(medium)
  df$combo <- log(df$t)^2 - 2*log(max(df$t))*log(df$t)
  # Requiring the vertex to be at the maximum time value introduces the constraint
  # b = -2c*log(t_max), which reduces the problem to the following:
  # log(y) ~ a + c*(log(t)^2 - 2*log(t_max)*log(t))
  model <- lm(log(y) ~ combo, data=df)
  return(model)
}

bipower_fit <- function(medium) {
  df <- make_df_for_fit(medium)
  
  # First approximation: r = 0.
  # log(y) ~ log(N*t^-p) = log(N)-p*log(t)
  model0 <- lm(log(y) ~ log(t), data=df)
  N0 <- exp(coef(model0)[1])
  p0 <- -coef(model0)[2]

  # Second approximation: q = 0.
  model1 <- nls(log(y) ~ log( N/(p+r) * (p*t^-(p+r) + r) ),
                start=list(p=p0, N=N0, r=0),
                data=df, control=list(maxiter=1e8), algorithm='port')
  N1 <- coef(model1)['N']
  p1 <- coef(model1)['p']
  r1 <- coef(model1)['r']

  # Full model.
  model <- nls(log(y) ~ log( N/(p-q+r) * ((p-q)*t^-(p+r) + r*t^-q) ),
               start=list(p=p1, N=N1, r=r1, q=0),
               data=df, control=list(maxiter=1e8), algorithm='port', lower=c(q=0))
  return(model)
}

power_fit <- function(medium) {
  df <- make_df_for_fit(medium)
  # log(y) ~ a + b*log(t)
  model <- lm(log(y) ~ log(t), data=df)
  return(model)
}

log_fit <- function(medium) {
  df <- make_df_for_fit(medium)
  # y ~ a + b*log(t)
  model0 <- lm(y ~ log(t), data=df)
  a0 <- coef(model0)[1]
  b0 <- coef(model0)[2]
  # Necessary to avoid negative argument to log in nls fit (for TWITTER only).
  if (a0 + b0*log(max(df$t)) <= 0) a0 <- -b0*log(max(df$t)) + 1e-8
  # log(y) ~ log(a + b*log(t))
  model <- nls(log(y) ~ log(a + b*log(t)),
               # Without "/2", we fail to find an optimum in the case of TWITTER.
               start=list(a=a0/2, b=b0/2),
               data=df, control=list(maxiter=1e8))
  return(model)
}

hyperbolic_fit <- function(medium) {
  df <- make_df_for_fit(medium)
  # 1/y ~ a + b*t
  model0 <- lm(1/y ~ t, data=df)
  a0 <- coef(model0)[1]
  b0 <- coef(model0)[2]
  # log(y) ~ -log(a + b*t)
  model <- nls(log(y) ~ -log(a + b*t),
               start=list(a=a0, b=b0),
               data=df, control=list(maxiter=1e8), algorithm='port')
  return(model)
}

hyperbolic_power_fit <- function(medium) {
  df <- make_df_for_fit(medium)
  # Start with a good guess.
  # 1/y ~ a + b*t^c
  c0 <- -7
  model0 <- lm(1/y ~ I(t^c0), data=df)
  a0 <- coef(model0)[1]
  b0 <- coef(model0)[2]
  # log(y) ~ -log(a + b*t^c)
  model <- nls(log(y) ~ -log(a + b*t^c),
               start=list(a=a0, b=b0, c=c0),
               data=df, control=list(maxiter=1e9, warnOnly=TRUE), algorithm='port', trace=FALSE,
               lower=c(a=0, b=-Inf, c=-Inf), upper=c(a=Inf, b=0, c=0))
  return(model)
}

plot_avg_fraction_of_mentioning_docs <- function(medium, model=NULL, model_type=NULL) {
  if (SAVE_PLOTS) pdf(sprintf('%s/avg_mention_curve_%s.pdf', PLOTDIR, medium), width=3.4, height=3.4,
                      pointsize=8, family='Helvetica', useDingbats=FALSE)
  par(mar=c(3,3,1,1))
  if (medium == 'NEWS') {
    x <- x_N
  } else if (medium == 'TWITTER') {
    x <- x_T
  } else {
    stop("medium must be NEWS or TWITTER!")
  }
  max_day <- 400
  days <- -100:max_day
  eps <- min(x[x>0], na.rm=TRUE)

  # Log.
  y <- colMeans(log10(x[,colnames(x) %in% days] + eps), na.rm=TRUE)
  par(fig=c(0, 1, 0, 1))
  plot(days, y, type='p', bty='n', lwd=2, xlab='', ylab='', #xaxt='n', yaxt='n',
       panel.first=abline(v=0, col='gray', lwd=1, lty=2), col=COL[[medium]], las=0)
  mtext('Days since death', side=1, line=2)
  mtext('Fraction mentioning documents [log10]', side=2, line=2)
  y365 <- y['365']
  arrows(x0=325, x1=355, y0=y365, y1=y365, length=0.05)
  text(325, y365, '365 days', adj=c(1.1, 0.5))

  if (!is.null(model)) {
    coefs <- coef(model)
    N <- coefs['N']
    p <- coefs['p']
    r <- coefs['r']
    q <- coefs['q']

    t <- 1:max_day
    if (model_type == 'Biexponential') {
      mem_comm <- N * exp(-(p+r)*t)
      mem_cult <- N/(p-q+r)*r * (exp(-q*t) - exp(-(p+r)*t))
    } else if (model_type == 'Bipower') {
      mem_comm <- N * t^-(p+r)
      mem_cult <- N/(p-q+r)*r * (t^-q - t^-(p+r))
    } else {
      stop("model_type must be 'biexp' or 'bipower'!")
    }
    mem <- mem_comm + mem_cult
    
    lines(t, log10(mem_comm), lty=1, lwd=2, col=COL_GREEN)
    lines(t, log10(mem_cult), lty=1, lwd=2, col=COL_RED)
    lines(t, log10(mem), lty=1, lwd=2, col='black')

    legend('topright', bty='n',
           legend=c('Daily average', sprintf('%s fit S(t) = u(t) + v(t)', model_type),
                    'Communicative memory u(t)', 'Cultural memory v(t)'),
           seg.len=3,
           pch=c(1,NA,NA,NA),
           lty=c(0,1,1,1), #lty=c(0,1,4,2),
           col=c(COL[[medium]], 'black', COL_GREEN, COL_RED),
           lwd=c(2, 2, 2, 2))
    
    # Log-log.
    par(fig=c(0.35, 0.9, 0.27, 0.77), new=TRUE)
    y <- y[names(y) %in% t]
    # Here the day of death is shown as day 1 (due to log axes).
    plot(t, y, type='p', bty='n', xlab='', ylab='', lwd=2, col=COL[[medium]], las=0, log='x')
    lines(t, log10(mem_comm), lty=1, lwd=2, col=COL_GREEN)
    lines(t, log10(mem_cult), lty=1, lwd=2, col=COL_RED)
    lines(t, log10(mem), lty=1, lwd=2, col='black')
    legend('topright', bty='n', legend='Log x-scale', inset=c(0.1,0.2))
  }

  if (SAVE_PLOTS) dev.off()
}

plot_model_fit <- function(medium, name, model, predict, log='') {
  if (medium == 'NEWS') {
    x <- x_N
  } else if (medium == 'TWITTER') {
    x <- x_T
  } else {
    stop("medium must be NEWS or TWITTER!")
  }
  
  df <- make_df_for_fit(medium)
  t <- df$t
  y <- log10(df$y)
  yhat <- log10(predict)
  
  par(mar=c(3,3,1,1))
  plot(t, y, type='p', bty='n', lwd=2, xlab='', ylab='', col=COL[[medium]], las=0, log=log)
  mtext('Days since death', side=1, line=2, cex=0.6)
  mtext('Mention freq. [log10]', side=2, line=2, cex=0.6)
  lines(t, yhat, lwd=2)

  aic <- AIC(model)
  r2 <- cor(yhat, y)^2
  if (log=='x') text(400, max(y),
                      sprintf('R² = %.3f\nAIC = %.0f', r2, aic),
                      adj=c(1,1), cex=1)
  else text(200, max(y), name, adj=c(0.5,1), cex=1.2)
}

plot_all_model_fits <- function(medium) {
  models <- list(
                 "Exponential"=exp_fit(medium),
                 "Hyperbolic"=hyperbolic_fit(medium),
                 "Logarithmic"=log_fit(medium),
                 "Power"=power_fit(medium),
                 "Hyperbolic-\npower"=hyperbolic_power_fit(medium),
                 "Log-normal-\nbased"=lognorm_constrained_fit(medium),
                 "Exponential-\npower"=exp_power_fit(medium),
                 "Biexponential"=biexp_fit(medium),
                 "Bipower"=bipower_fit(medium)
                 )
  if (SAVE_PLOTS) pdf(sprintf('%s/all_model_fits_%s.pdf', PLOTDIR, medium),
                      width=3.5, height=9, pointsize=8, family='Helvetica', useDingbats=FALSE)
  par(mfcol=c(9,2))
  for (log in c('', 'x')) {
    for (name in names(models)) {
      model <- models[[name]]
      plot_model_fit(medium, name, model, exp(predict(model)), log=log)
    }
  }
  if (SAVE_PLOTS) dev.off()
}

plot_comm_vs_cult_mem <- function(medium, coefs, model_type) {
  if (SAVE_PLOTS) pdf(sprintf('%s/comm_vs_cult_mem_%s.pdf', PLOTDIR, medium), width=3.4, height=2.4,
                      pointsize=8, family='Helvetica', useDingbats=FALSE)
  par(mar=c(4,4,1,1.5))
  if (medium == 'NEWS') {
    x <- x_N
  } else if (medium == 'TWITTER') {
    x <- x_T
  } else {
    stop("medium must be NEWS or TWITTER!")
  }
  max_day <- 150
  t <- seq(1, max_day, 0.01)
  N <- coefs['N']
  p <- coefs['p']
  r <- coefs['r']
  q <- coefs['q']
  if (model_type == 'Biexponential') {
    u <- function(t, N, p, r, q) N * exp(-(p+r)*t)
    v <- function(t, N, p, r, q) N/(p-q+r)*r * (exp(-q*t) - exp(-(p+r)*t))
  } else if (model_type == 'Bipower') {
    u <- function(t, N, p, r, q) N * t^-(p+r)
    v <- function(t, N, p, r, q) N/(p-q+r)*r * (t^-q - t^-(p+r))
  } else {
    stop("model_type must be 'biexp' or 'bipower'!")
  }
  mem_comm <- u(t, N, p, r, q)
  mem_cult <- v(t, N, p, r, q)
  ratio <- mem_cult / (mem_comm + mem_cult)
  plot(t, ratio, type='l', bty='n', col=COL[[medium]], lwd=2, ylim=c(0,1), axes=FALSE,
       xlab='Days since death', ylab='Fraction of cultural memory')
  axis(1)
  axis(2, at=seq(0,1,0.25))
  percs <- c(0.25, 0.5, 0.75)
  cols <- c('black', COL_MAGENTA, COL_LIGHTBLUE)
  for (i in 1:length(percs)) {
    perc <- percs[i]
    col <- cols[i]
    idx <- min(which(ratio>=perc))
    points(t[idx], ratio[idx], lwd=3, pch=20, col=col, cex=2)
    segments(-5, ratio[idx], t[idx], ratio[idx], col=col, lty=2)
    segments(t[idx], -5, t[idx], ratio[idx], col=col, lty=2)
    text(t[idx], ratio[idx], adj=c(-0.1, 0.5), col=col,
         labels=sprintf('%.0f%% after %.0f days', 100*perc, ceiling(t[idx])))
  }
  if (SAVE_PLOTS) dev.off()
}

overlay_time_series <- function(X, medium) {
    matplot(-N:N, t(X), type='l', lty=1, lwd=2, col=COL_XLIGHT[[medium]], xlab='Days since death',
            ylab='Fraction mentioning docs [log10]', ylim=range(X), main=medium, bty='n')
}

smoothed_density <- function(x, bw=0.05, range=c(0,1)) {
  # Reflect the data to avoid drops at boundaries.
  left <- -(x-range[1]) + range[1]
  right <- left + 2*(range[2]-range[1])
  x <- c(left, x, right)
  est <- bkde(x, bandwidth=bw, range=range(x), gridsize=1000)
  # Consider only the original range.
  idx <- which(est$x >= range[1] & est$x <= range[2])
  est$x <- est$x[idx]
  # Renormalize.
  est$y <- 3*est$y[idx]
  est
}

# Smoothed histograms of curve properties for the different people groups.
plot_smoothed_densities_raw <- function(stats, medium, prop, groups, main=NULL) {
  nbw <- 20
  chars <- c('mean_before', 'peak_mean_boost', 'perm_boost', 'time_till_half')
  par(mfrow=c(length(groups), length(chars)), mar=c(4,4,2,2))
  for (char in chars) {
    # Find the extreme y-value to be plotted.
    ymin <- Inf
    ymax <- -Inf
    for (group in groups) {
      mask <- is.finite(stats[,prop]) & stats[,prop] %in% group
      x <- stats[mask,char]
      sm <- smoothed_density(x, bw=(max(x)-min(x))/nbw, range=range(x))$y
      ymin <- min(ymin, min(sm))
      ymax <- max(ymax, max(sm))
    }
    for (group in groups) {
      mask <- is.finite(stats[,prop]) & stats[,prop] %in% group
      x <- stats[mask,char]
      est <- smoothed_density(x, bw=(max(x)-min(x))/nbw, range=range(x))
      # The estimate without reflected data, that is it drops at the boundaries.
      est_ <- bkde(x, bandwidth=(max(x)-min(x))/nbw, range=range(x))
      plot(est$x, est$y, ylim=c(ymin,ymax), type='l', lwd=2,
           main=if (is.null(main)) group else main,
           col=COL[[medium]], xlab=FANCY_CURVE_CHAR_NAMES[char], ylab='Density', bty='n')
      rug(x)
    }
  }
  par(.default_par)
}

plot_chars_by_group <- function(stats, medium, prop, groups) {
  chars <- c('mean_before', 'peak_mean_boost', 'perm_boost', 'time_till_half')
  par(mfrow=c(1, length(chars)), mar=c(10,4,2,2))
  for (char in chars) {
    char_rank <- sprintf('%s_rank', char)
    grouped <- split(stats[,char_rank]/nrow(stats), prop)[as.character(groups)]
    boxplot(grouped, las=3, notch=TRUE, main=FANCY_CURVE_CHAR_NAMES[char],
            col=COL_LIGHT[[medium]], bty='n', ylab='Relative rank')
  }
  par(.default_par)
}

smoothed_2d_density <- function(x, bw) {
  # Reflect the data to avoid drops at boundaries.
  x <- rbind(cbind( -x[,1],  -x[,2]),
             cbind( -x[,1],   x[,2]),
             cbind( -x[,1], 2-x[,2]),
             cbind(  x[,1],  -x[,2]),
             cbind(  x[,1],   x[,2]),
             cbind(  x[,1], 2-x[,2]),
             cbind(2-x[,1],  -x[,2]),
             cbind(2-x[,1],   x[,2]),
             cbind(2-x[,1], 2-x[,2]))
  est <- bkde2D(x, range.x=list(c(-1,2), c(-1,2)), bandwidth=c(bw,bw), gridsize=c(153,153))
  # Consider only the original range [0,1].
  idx <- which(est$x1 >= 0 & est$x1 <= 1)
  est$x1 <- est$x1[idx]
  est$x2 <- est$x2[idx]
  # Renormalize.
  est$fhat <- 9*est$fhat[idx,idx]
  est
}

plot_smoothed_2d_densities <- function(stats, medium, char1, char2, prop, groups, main=NULL) {
  bw <- 0.1
  char1_rank <- sprintf('%s_rank', char1)
  char2_rank <- sprintf('%s_rank', char2)
  # Find the maximum y-value to be plotted.
  ymax <- -Inf
  for (group in groups) {
    mask <- is.finite(stats[,prop]) & stats[,prop] %in% group
    x <- cbind(stats[mask, char1_rank], stats[mask, char2_rank])/length(mask)
    ymax <- max(ymax, max(smoothed_2d_density(x, bw=bw)$fhat))
  }
  for (group in groups) {
    mask <- is.finite(stats[,prop]) & stats[,prop] %in% group
    x <- cbind(stats[mask, char1_rank], stats[mask, char2_rank])/length(mask)
    est <- smoothed_2d_density(x, bw=bw)
    filled.contour(est$x1, est$x2, est$fhat, zlim=c(0,ymax),
                   color=function(n) gray.colors(n,0,1),
                   plot.title=title(main=sprintf('%s\n(%s)', if (is.null(main)) group else main, medium),
                                    xlab=char1, ylab=char2))
    par(.default_par)
  }
}

cluster <- function(stats, feats) {
  input <- cbind(stats[,feats])
  colnames(input) <- feats
  # z-score normalize.
  input <- t(t(input) - colMeans(input))
  input <- t(t(input) / apply(input, 2, sd))
  K <- 2:30
  kmeans_all <- mclapply(K, function(k) { set.seed(1); kmeans(input, k, nstart=20) })
  # Reorder the clusters.
  for (k in K) {
    km <- kmeans_all[[which(K==k)]]
    ord <- order(km$size, decreasing=TRUE)
    ord_map <- 1:k; names(ord_map) <- ord
    km$size <- km$size[ord]
    km$withinss <- km$withinss[ord]
    km$centers <- km$centers[ord,]
    rownames(km$centers) <- 1:k
    km$cluster <- ord_map[as.character(km$cluster)]
    kmeans_all[[which(K==k)]] <- km
  }
  list(clustering=kmeans_all, diss=dist(input), K=K)
}

plot_sil_width <- function(medium, kmeans_all, diss) {
  avg_sil_width_kmeans <- unlist(lapply(kmeans_all, function(km) summary(silhouette(km$cluster, diss^2))$avg.width))
  plot(2:(length(kmeans_all)+1), avg_sil_width_kmeans, type='b', xlab='Number of clusters',
       ylab='Avg. silhouette width', main=medium, col=COL[[medium]], lwd=2, bty='n')
}

plot_clustered_time_series <- function(medium, kmeans, X) {
  k <- nrow(kmeans$centers)
  par(mfrow=c(1, k))
  for (j in 1:k) {
    matplot(-N:N, t(X[kmeans$cluster==j,]), type='l', lty=1, lwd=2, col=COL_XLIGHT[[medium]],
            xlab='Days since death', ylab='% mentioning docs [log10]', ylim=range(X),
            main=sprintf('%s cluster %s', medium, j))
  }
  par(.default_par)
}

boot_ci <- function(x, func, R=5000) {
  bo <- boot(x, statistic=function(d, i) func(d[i]), R=R)
  ci <- boot.ci(bo, conf=0.95, type="basic")$basic[4:5]
  if (is.null(ci)) {
    upper <- lower <- NA
  } else {
    lower <- ci[1]
    upper <- ci[2]
  } 
  unlist(list(upper=upper, point_est=func(x), lower=lower))
}

make_regression_data <- function(medium) {
  if (medium != 'NEWS' && medium != 'TWITTER') stop('Medium must be \'NEWS\' or \'TWITTER\'')
  stats <- if (medium=='NEWS') stats_N else stats_T
  data <- stats[stats$gender != ''
                & stats$death_type != 'unknown'
                & !is.na(stats$type_group) # This removes only 4 people.
                & !is.na(stats$age_group)
                & stats$age %in% 20:99,]
  data$age_group <- factor(data$age_group)
  # Mean (median) age is 71 (74), so use the age bracket 70-79 as the reference level.
  base_age_group  <- '70'
  base_death_type <- 'unknown'
  base_death_type <- 'natural'
  base_type_group <- 'art' # 'general fame'
  base_gender     <- 'Male'
  base_anglo      <- 'anglo'
  data$age_group  <- relevel(data$age_group, base_age_group)
  data$death_type <- relevel(data$death_type, base_death_type)
  data$type_group <- relevel(data$type_group, base_type_group)
  data$gender     <- relevel(data$gender, base_gender)
  data$anglo      <- relevel(data$anglo, base_anglo)
  N <- dim(data)[1]
  relranks <- (0:(N-1))/(N-1) - 0.5
  data$mean_before_relrank[order(data$mean_before_rank)] <- relranks
  data$peak_mean_boost_relrank[order(data$peak_mean_boost_rank)] <- relranks
  data$perm_boost_relrank[order(data$perm_boost_rank)] <- relranks
  return(data)
}

make_regression_data_T_vs_N <- function() {
  lmdata <- make_regression_data('NEWS')
  lmdata$mean_before_relrank_diff     <- lmdata_N$mean_before_relrank - lmdata_T$mean_before_relrank
  lmdata$peak_mean_boost_diff         <- lmdata_N$peak_mean_boost - lmdata_T$peak_mean_boost
  lmdata$perm_boost_diff              <- lmdata_N$perm_boost - lmdata_T$perm_boost
  lmdata$peak_mean_boost_relrank_diff <- lmdata_N$peak_mean_boost_relrank - lmdata_T$peak_mean_boost_relrank
  lmdata$perm_boost_relrank_diff      <- lmdata_N$perm_boost_relrank - lmdata_T$perm_boost_relrank
  return(lmdata)
}

# The real uncertainty in the summed coefficients is larger; need to add separate SEs and entries of vcov
# matrix: https://biologyforfun.wordpress.com/2017/05/17/adding-standard-errors-for-interaction-terms/comment-page-1/
compound_se_for_lm <- function(mod, interact=NULL, base_age=NULL) {
  X <- as.data.frame(vcov(mod))
  idx <- rownames(X)[grep('age_group..$', rownames(X))]
  X[sprintf('age_group%s:%s', base_age, interact),] <- 0
  X[,sprintf('age_group%s:%s', base_age, interact)] <- 0
  sapply(idx, function(i) {
    sub_idx <- c(i, if (is.null(interact)) NULL else c(interact, sprintf('%s:%s', i, interact)))
    sqrt(sum(X[sub_idx, sub_idx]))
    })
}

run_regression_analysis <- function(medium, ranks) {
  if (medium != 'NEWS' && medium != 'TWITTER') stop('Medium must be \'NEWS\' or \'TWITTER\'')
  lmdata <- if (medium == 'NEWS') lmdata_N else lmdata_T
  predictors <- 'mean_before_relrank + age_group + death_type + gender + anglo + type_group'
  suffix <- if (ranks) '_relrank' else ''
  mod_peak <- lm(as.formula(sprintf('peak_mean_boost%s ~ %s', suffix, predictors)), lmdata)
  mod_perm <- lm(as.formula(sprintf(     'perm_boost%s ~ %s', suffix, predictors)), lmdata)
  return(list(mod_peak=mod_peak, mod_perm=mod_perm))
}

run_regression_analysis_for_death_types <- function(medium, ranks) {
  if (medium != 'NEWS' && medium != 'TWITTER') stop('Medium must be \'NEWS\' or \'TWITTER\'')
  lmdata <- if (medium == 'NEWS') lmdata_N else lmdata_T
  base_death_type <- levels(lmdata$death_type)[1]
  # Compare natural vs. unnatural death for the different age brackets (0 to omit the
  # intercept and include it instead as the coefficient of the base age).
  predictors <- '0 + mean_before_relrank + age_group + death_type + gender + anglo + type_group + age_group:death_type'
  interact <- sprintf('death_type%s', if (base_death_type=='natural') 'unnatural' else 'natural')
  suffix <- if (ranks) '_relrank' else ''
  mod_peak <- lm(as.formula(sprintf('peak_mean_boost%s ~ %s', suffix, predictors)), lmdata)
  mod_perm <- lm(as.formula(sprintf(     'perm_boost%s ~ %s', suffix, predictors)), lmdata)
  return(list(mod_peak=mod_peak, mod_perm=mod_perm))
}

run_regression_analysis_T_vs_N <- function(ranks) {
  lmdata <- make_regression_data_T_vs_N()
  predictors <- 'mean_before_relrank_diff + age_group + death_type + gender + anglo + type_group'
  suffix <- if (ranks) '_relrank' else ''
  mod_peak <- lm(as.formula(sprintf('peak_mean_boost%s_diff ~ %s', suffix, predictors)), lmdata)
  mod_perm <- lm(as.formula(sprintf('perm_boost%s_diff ~ %s', suffix, predictors)), lmdata)
  return(list(mod_peak=mod_peak, mod_perm=mod_perm))
}

run_regression_analysis_T_vs_N_for_death_types <- function(ranks) {
  lmdata <- make_regression_data_T_vs_N()
  predictors <- '0 + mean_before_relrank_diff + age_group + death_type + gender + anglo + type_group + age_group:death_type'
  suffix <- if (ranks) '_relrank' else ''
  mod_peak <- lm(as.formula(sprintf('peak_mean_boost%s_diff ~ %s', suffix, predictors)), lmdata)
  mod_perm <- lm(as.formula(sprintf('perm_boost%s_diff ~ %s', suffix, predictors)), lmdata)
  return(list(mod_peak=mod_peak, mod_perm=mod_perm))
}

plot_age_coeffs_for_death_types <- function(mod, medium, outcome, base_age, ranks,
                                            ylim=NULL, col=COL[[medium]]) {
  summary <- coef(summary(mod))

  # Non-interaction terms (i.e., natural death).
  idx <- grep('^age_group..$', rownames(summary))
  decades <- as.numeric(sub('.*(\\d\\d).*', '\\1', rownames(summary))[idx])
  ord <- order(decades)
  decades <- decades[ord]
  age_coef_natural <- summary[idx,1][ord]
  se_natural <- compound_se_for_lm(mod)[ord]

  # Interaction terms (i.e., unnatural death).
  idx <- grep('^age_group..:death_typeunnatural$', rownames(summary))
  summary[idx,1] <- summary[idx,1] + summary['death_typeunnatural',1]
  rownames(summary)[rownames(summary)=='death_typeunnatural'] <- sprintf('age_group%s:death_typeunnatural', base_age)
  # Add age_group baseline coefs to interaction coefs.
  idx <- grep('^age_group..:death_typeunnatural$', rownames(summary))
  summary[idx,1] <- summary[idx,1] + summary[grep('age_group..$', rownames(summary)), 1]
  age_coef_unnatural <- summary[idx,1][ord]
  se_unnatural <- compound_se_for_lm(mod, 'death_typeunnatural', base_age)[ord]

  if (is.null(ylim)) ylim <- range(c(age_coef_natural - 2*se_natural,
                                     age_coef_natural + 2*se_natural,
                                     age_coef_unnatural - 2*se_unnatural,
                                     age_coef_unnatural + 2*se_unnatural), na.rm=TRUE)
  col_unnat <- 'gray'
  suffix <- if (ranks) '_relrank' else ''
  if (outcome == 'peak_mean_boost') {
    main <- sprintf('Short-term boost', medium)
    xlab <- 'Average boost'
    outfile <- sprintf('%s/age_coefs_%s_%s%s.pdf', PLOTDIR, medium, outcome, suffix)
  } else if (outcome == 'perm_boost') {
    main <- sprintf('Long-term boost', medium)
    xlab <- 'Average boost'
    outfile <- sprintf('%s/age_coefs_%s_%s%s.pdf', PLOTDIR, medium, outcome, suffix)
  } else if (outcome == 'peak_mean_boost_diff') {
    main <- 'Short-term boost'
    xlab <- 'Average boost difference'
    outfile <- sprintf('%s/age_coefs_NEWS-VS_TWITTER_%s%s.pdf', PLOTDIR, outcome, suffix)
  } else if (outcome == 'perm_boost_diff') {
    main <- 'Long-term boost'
    xlab <- 'Average boost difference'
    outfile <- sprintf('%s/age_coefs_NEWS-VS_TWITTER_%s%s.pdf', PLOTDIR, outcome, suffix)
  } else {
    stop('Illegal outcome name')
  }
  
  if (SAVE_PLOTS) cairo_pdf(outfile, width=3.4, height=2, pointsize=8, family='Helvetica')
  par(mar=c(3,3,1,1))
  plot(decades+6, age_coef_unnatural, type='b', panel.first=abline(h=0, col='gray'),
       ylim=ylim, xlim=c(min(decades), max(decades)+10), bty='n', xaxt='n', yaxt='n',
       main=main, xlab='', ylab='', lwd=2, col=col_unnat)
  dispersion(decades+6, age_coef_unnatural, 2*se_unnatural, col=col_unnat, arrow.cap=0.005)
  axis(1, at=c(decades, max(decades)+10), line=0)
  axis(2, line=0)
  mtext('Age at death', 1, line=2)
  mtext(xlab, 2, line=2)
  points(decades+4, age_coef_natural, type='b', lwd=2, col=col)
  dispersion(decades+4, age_coef_natural, 2*se_natural, col=col, arrow.cap=0.005)
  legend('bottomleft', c('Natural death', 'Unnatural death'), col=c(col, col_unnat), lty=1, lwd=2, horiz=TRUE, bty='n')
  if (SAVE_PLOTS) dev.off()
}

print_regression_table <- function(mods_N, mods_T, ranks) {
  age_names <- sprintf("Age: %d--%d", seq(20,90,10)[-6], seq(29,99,10)[-6])
  type_names <- levels(lmdata_N$type_group)[-1]
  table <- texreg(list(mods_N$mod_peak, mods_T$mod_peak, mods_N$mod_perm, mods_T$mod_perm), dcolumn=TRUE, booktabs=TRUE,
                  use.packages=FALSE, digits=3, leading.zero=FALSE, single.row=TRUE, stars = c(0.001,0.01,0.05),
                  custom.model.names=c("\\shortstack{Short-term boost\\\\(News)}",
                                       "\\shortstack{Short-term boost\\\\(Twitter)}",
                                       "\\shortstack{Long-term boost\\\\(News)}",
                                       "\\shortstack{Long-term boost\\\\(Twitter)}"),
                  custom.coef.names=c("(Intercept)", "Pre-mortem mean (relative rank)", age_names, "Manner of death: unnatural",
                                      "Gender: female", paste("Language:", c("non-anglophone", "unknown")),
                                      paste("Notability type:", type_names)),
                  reorder.coef=c(1,2,10,12:13,11,14:18,3:9), table=FALSE)
  cat(table, file=sprintf('%s/lm_coef_T_and_N%s.tex', TABLEDIR, if (ranks) '_relrank' else ''), sep="\n")
}

print_regression_table_T_vs_N <- function(mods, ranks) {
  age_names <- sprintf("Age: %d--%d", seq(20,90,10)[-6], seq(29,99,10)[-6])
  type_names <- levels(lmdata_N$type_group)[-1]
  table <- texreg(list(mods$mod_peak, mods$mod_perm), dcolumn=TRUE, booktabs=TRUE,
                  use.packages=FALSE, digits=3, leading.zero=FALSE, single.row=TRUE, stars = c(0.001,0.01,0.05),
                  label = "table:coefficients News vs. Twitter", custom.model.names=c("Short-term boost", "Long-term boost"),
                  custom.coef.names=c("(Intercept)", "Pre-mortem mean (relative-rank diff.)", age_names,
                                      "Manner of death: unnatural",
                                      "Gender: female", paste("Language:", c("non-anglophone", "unknown")),
                                      paste("Notability type:", type_names)),
                  reorder.coef=c(1,2,10,12:13,11,14:18,3:9), table=FALSE)
  cat(table, file=sprintf('%s/lm_coef_T_vs_N%s.tex', TABLEDIR, if (ranks) '_relrank' else ''), sep="\n")
}

print_model_description <- function(medium, depvar, ranks) {
  cat('\n\n#####################################################\n')
  cat('REGRESSION MODEL\n')
  cat(sprintf('Medium: %s\n', medium))
  cat(sprintf('Dependent variable: %s\n', depvar))
  cat(sprintf('Transformation on dependent variable: %s\n', if (ranks) 'RELATIVE RANKS' else 'NONE'))
  cat('#####################################################\n')
}

load_doc_length_regression_data <- function() {
  file <- sprintf('%s/RData/log_doc_length_ratios.RData', DATADIR)
  if (!file.exists(file)) {
    # Note: this uses all English non-Twitter documents, not only those from the 6608 Google News
    # domains. Also, it uses only a set of 2638 people that were used in an early stage of the project
    # (meeting the criterion "min_num_active_days=50"), which are then reduced to the subset
    # of 579 people that are also present in the set of 870 people included in the main regression
    # analysis about post-mortem mention frequency.
    doc_len_all <- read.table(pipe(sprintf('gunzip -c %s/mean_doc_length_per_day.tsv.gz', DATADIR)),
                              sep='\t', as.is=TRUE, col.names=c('mid', 'date', 'numWords'))
    ok_mids <- intersect(unique(doc_len_all$mid), unique(rownames(lmdata_N)))
    doc_len <- doc_len_all[doc_len_all$mid %in% ok_mids,]
    doc_len$rel_date <- as.numeric(as.Date(doc_len$date) - as.Date(death_dates[doc_len$mid]))
    feature <- 'numWords'
    
    x <- do.call(rbind, mclapply(split(doc_len, doc_len$mid), function(l) {
      dod <- as.Date(death_dates[l$mid[1]])
      offset <- as.numeric(MAX_DATE-MIN_DATE) + 1
      y <- rep(NA, NRELDATES)
      # Log.
      y[l$rel_date + offset] <- log(l[,feature])
      y[as.numeric(as.Date(EMPTY_DAYS)-dod) + offset] <- NA
      return(y)
    }, mc.cores=6))
    extreme <- (NRELDATES-1)/2
    colnames(x) <- -extreme:extreme
    
    x <- x[, colnames(x) %in% -100:N]
    premeans <- rowMeans(x[,colnames(x) <= -30], na.rm=TRUE)
    x_norm <- x - premeans
    regdata <- cbind(lmdata_N[rownames(x_norm),], x_norm)
    save(regdata, file=file)
  } else {
    load(file)
  }
  return(regdata)
}

plot_doc_length_increase <- function(regdata, predictors, ylab=NULL) {
  coefs <- do.call(rbind,
                   lapply(-100:N,
                          function(t) summary(lm(as.formula(sprintf('`%s` ~ %s', t, predictors)),
                                                 regdata))$coefficients[1,1:2]))
  est <- coefs[,1]
  lo <- coefs[,1]-2*coefs[,2]
  hi <- coefs[,1]+2*coefs[,2]

  plot(-100:N, exp(est)-1, type='l', lwd=2, col='#000000', bty='n', axes=FALSE,
       ylim=c(-0.7,0.7),xlab='Days since death', ylab=ylab,
       main='Mean relative document length increase', cex.main=1,
       panel.first=c(abline(v=0, h=0, col='gray'), #abline(v=c(30), col='red'),
                     polygon(x=c(-100:N, rev(-100:N)), y=c(lo, rev(hi)), col='#00000040', border=NA),
                     axis(1, at=seq(-90,360,30), las=2), axis(2)))
}

draw_mega_figure <- function() {
  days <- -100:360
  feat_cols <- c(mean_before=COL_DARKBLUE, peak_mean_boost=COL_MAGENTA, perm_boost=COL_GREEN, time_till_half=COL_RED)
  bg <- function(col) rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col=col, border=NA)
  mar <- 0.2
  
  if (SAVE_PLOTS) cairo_pdf(sprintf('%s/megafigure.pdf', PLOTDIR), width=8, height=8, pointsize=6, family='Helvetica')
  
  par(.default_par)
  plot.new()
  rel_line_height <- par('mai')[1]/par('mar')[1]/par('din')
  par(fig=c(0.59, 0.91, 0.09, 0.41), new=TRUE)
  par(mar=c(0,0,0,0))
  confusion <- table(as.factor(km_N$cluster), as.factor(km_T$cluster))
  palmat <- color.scale(confusion^0.5, cs1=c(1,1), cs2=c(1,0.2), cs3=c(1,0.2))
  # Must fix the function definition of color2D.matplot: do fix(color2D.matplot) and replace "f" with "d" on line 101:
  # https://stackoverflow.com/questions/44522895/plot-color-matrix-with-numbers-in-r
  color2D.matplot(confusion, cellcolors=palmat, main='', show.values=1, vcol=rgb(0,0,0), axes=FALSE, vcex=1.2)
  feats <- c('mean_before', 'peak_mean_boost', 'perm_boost', 'time_till_half')
  # bar_range <- 1.1 * range(c(km_N$centers, km_N$centers))
  bar_range <- c(-2.1, 5.2)
  for (medium in c('NEWS', 'TWITTER')) {
    if (medium == 'NEWS') {
      kmeans <- km_N
      X <- X_N
      x <- x_N
      fig <- function(j) c(0.51, 0.59, (4-j)*0.08+0.09, (5-j)*0.08+0.09)
      fig_incr <- c(mar*rel_line_height[1], mar*rel_line_height[1], 0, 0)
      fig_bar <- function(j) c(0.91, 0.99, (4-j)*0.08+0.09, (5-j)*0.08+0.09)
    } else {
      kmeans <- km_T
      X <- X_T
      x <- x_T
      fig <- function(j) c(j*0.08+0.51, (j+1)*0.08+0.51, 0.41, 0.49)
      fig_incr <- c(0, 0, -mar*rel_line_height[2], -mar*rel_line_height[2])
      fig_bar <- function(j) c(j*0.08+0.51, (j+1)*0.08+0.51, 0.01, 0.09)
    }
    cluster_sizes <- kmeans$size
    y <- do.call(cbind, lapply(1:4, function(j) colMeans(X[kmeans$cluster==j, colnames(X) %in% days], na.rm=TRUE)))
    for (j in 1:4) {
      # bar_range <- 1.1 * range(kmeans$centers)
      par(fig=fig(j)-fig_incr, new=TRUE, mar=c(mar, mar, mar, mar))
      plot(days, y[,j], type='l', ylim=range(y), las=2, col=COL[[medium]], lwd=2, xlab='', ylab='', xaxt='n', yaxt='n',
           bty='n', panel.first=bg("#eeeeee"))
      text(max(days), max(y), sprintf('C%d', j), adj=c(1,1), cex=1)
      par(fig=fig_bar(j)+fig_incr, new=TRUE, mar=c(mar, mar, mar, mar))
      barplot(kmeans$centers[j,], main='', ylab='', col=feat_cols, names.arg='', ylim=bar_range, yaxt='n', border=NA,
              bty='n', panel.first=bg("#eeeeee"))
      if (medium=='TWITTER' && j==4) {
        mtext('z-scores', side=4, line=1.8, cex=0.5)
        axis(4, at=seq(-2,5), labels=c('-2','','0','','2','','4',''), cex.axis=0.5, cex.lab=0.5,
                                          lwd=0.8, lwd.ticks=0.8, las=1, hadj=0)
      }
      text(par("usr")[2]*0.95, par("usr")[4]*0.95, adj=c(1,1), cex=0.5,
           sprintf('N=%d\n(%.0f%%)', kmeans$size[j], 100*kmeans$size[j]/sum(kmeans$size)))
    }
  }
  ### Matrix row label.
  par(fig=c(0.50, 1, 0.50, 0.55), new=TRUE, mar=c(0,0,0,0))
  plot(NULL, xaxt='n', yaxt='n', bty='n', ylab='', xlab='', xlim=0:1, ylim=0:1)
  text(0.5, 0, 'Twitter', adj=c(0.5,0), col=COL[['TWITTER']], cex=1.2)
  ### Matrix column label.
  par(fig=c(0.40, 0.59, 0.42, 0.50), new=TRUE, mar=c(0,0,0,0))
  plot(NULL, xaxt='n', yaxt='n', bty='n', ylab='', xlab='', xlim=0:1, ylim=0:1)
  text(1, 0, 'News', adj=c(1,0), col=COL[['NEWS']], cex=1.2)
  
  ### Barplot legend.
  par(fig=c(0.45, 0.59, 0.01, 0.09), new=TRUE)
  plot(NULL, xaxt='n', yaxt='n', bty='n', ylab='', xlab='', xlim=0:1, ylim=0:1)
  legend("bottomleft", legend=c(' pre-mortem mean', ' short-term boost', ' long-term boost', ' halving time'),
         pch=15, pt.cex=3, cex=0.7, y.intersp=1.7,  bty='n', col=feat_cols, text.col=feat_cols)
  
  for (i in 3:1) {
    delta <- 0.007
    par(fig=c(i*delta, 0.4+i*delta, i*delta, 0.4+i*delta), new=TRUE, mar=c(5,5,0,0))
    plot(NULL, xaxt='n', yaxt='n', xlab='', ylab='', panel.first=bg('white'), xlim=0:1, ylim=0:1)
  }
  par(fig=c(0, 0.50, 0.43, 0.59), new=TRUE, mar=c(0,0,0,0))
  plot(NULL, xaxt='n', yaxt='n', bty='n', ylab='', xlab='', xlim=0:1, ylim=0:1)
  text(0.5, 0, sprintf('Mention time series (N=%d)', dim(X)[1]), adj=c(0.5,0), cex=0.8)
  
  par(fig=c(0, 0.40, 0, 0.40), new=TRUE, mar=c(5,5,0,0))
  name <- 'Whitney_Houston'
  # name <- 'Donna_Summer'
  # name <- 'Amy_Winehouse'
  medium <- 'NEWS'
  if (medium == 'NEWS') {
    X <- X_N
    x <- x_N
    num_art <- num_art_N
  } else {
    X <- X_T
    x <- x_T
    num_art <- num_art_T
  }
  cex <- 0.65 # 0.85
  mid <- wiki_to_mid[sprintf('<http://en.wikipedia.org/wiki/%s>', name)]
  raw <- log10(x[mid, colnames(x) %in% days])
  smoothed <- X[mid, colnames(X) %in% days]
  ymin <- min(raw[is.finite(raw)], na.rm=TRUE)
  col <- as.vector(col2rgb(COL[[medium]])/255)
  col <- rgb(col[1], col[2], col[3], 0.2)
  plot(days, raw, col=col, main='', pch=20, cex=2, cex.axis=0.7, cex.lab=0.7,
       bty='o', xlab='Days since death', ylab='Fraction mentioning documents [log10]', ylim=c(ymin,-1),
       panel.first=c(bg('white'), abline(v=0, col='gray', lty=2)))
  lines(names(smoothed), smoothed, col=COL[[medium]], lwd=2)
  text(max(days), -1, sprintf('%s', gsub('_', ' ', name)), adj=c(1,1), cex=1) # cex=1.5)
  y_before <- stats_N[mid, 'mean_before']
  y_peak <- y_before + stats_N[mid, 'peak_mean_boost']
  y_perm <- y_before + stats_N[mid, 'perm_boost']
  x_tth <- stats_N[mid, 'time_till_half'] * max(as.numeric(colnames(X)))
  y_tth <- mean(c(y_peak, y_perm))
  # mean before
  segments(x0=min(days), y0=y_before, x1=-30, lwd=2, col=feat_cols['mean_before'])
  text(-60, y_before, 'pre-mortem\nmean', srt=0, adj=c(0.5,1.5), col=feat_cols['mean_before'], cex=cex)
  # peak
  segments(x0=0, y0=y_peak, x1=30)
  arrows(x0=-40, y0=y_before, y1=y_peak, length=0.05, code=3, lwd=2, col=feat_cols['peak_mean_boost'])
  text(-70, mean(c(y_before, y_peak)), 'short-\nterm\nboost', srt=0, adj=c(0.5,0.5), col=feat_cols['peak_mean_boost'], cex=cex)
  # perm
  segments(x0=30, y0=y_perm, x1=max(days))
  arrows(x0=100, y0=y_before, y1=y_perm, length=0.05, code=3, lwd=2, col=feat_cols['perm_boost'])
  text(110, mean(c(y_before, y_perm)), 'long-term boost', srt=0, adj=c(0,0.5), col=feat_cols['perm_boost'], cex=cex)
  # tth
  arrows(x0=1, y0=y_tth, x1=x_tth, length=0.05, code=3, lwd=2, col=feat_cols['time_till_half'])
  text(x_tth+10, y_tth, 'halving time', srt=0, adj=c(0,0.5), col=feat_cols['time_till_half'], cex=cex)
  
  ### Arrow from example to cluster.
  par(fig=c(0.40, 0.51, 0, 0.45), new=TRUE, mar=c(0,0,0,0))
  plot(NULL, xaxt='n', yaxt='n', bty='n', xlab='', ylab='', xlim=0:1, ylim=0:1)
  col <- 'gray50'
  arrows(x0=0, y0=0.5, x1=0.5, length=0.05, code=0, lwd=5, col=col, ljoin=1, lend=2)
  arrows(x0=0.5, y0=0.5, y1=0.40, length=0.05, code=0, lwd=5, col=col, ljoin=1, lend=2)
  arrows(x0=0.5, y0=0.40, x1=0.98, length=0.2, code=2, lwd=5, col=col, ljoin=1, lend=1)
  text(0.50, 0.53, 'Clustering', srt=0, adj=c(0,0.5), col=col, cex=1.5, font=2, srt=90)
  
  if (SAVE_PLOTS) dev.off()
}

num_art_N <- get_num_art('NEWS')
num_art_T <- get_num_art('TWITTER')

data_N <- get_mention_freq_table('NEWS')
data_T <- get_mention_freq_table('TWITTER')

x_N <- get_rel_date_matrix('NEWS', data_N, num_art_N, CHUNK_SIZE)
x_T <- get_rel_date_matrix('TWITTER', data_T, num_art_T, CHUNK_SIZE)

mids_N <- filter_people('NEWS', x_N)
mids_T <- filter_people('TWITTER', x_T)
mids <- intersect(mids_N, mids_T)

x_N <- x_N[mids,]
x_T <- x_T[mids,]

X_N <- normalize_and_smooth('NEWS', x_N, num_art_N, mean_center=FALSE)
X_T <- normalize_and_smooth('TWITTER', x_T, num_art_T, mean_center=FALSE)

stats_all <- load_stats_all()

stats_N <- load_stats('NEWS', x_N, X_N, num_art_N)
stats_T <- load_stats('TWITTER', x_T, X_T, num_art_T)

lmdata_N <- make_regression_data('NEWS')
lmdata_T <- make_regression_data('TWITTER')

BASE_AGE <- levels(lmdata_N$age_group)[1]

# stats_N and stats_T contain the same people, so we arbitrarily use stats_N here.
natural <- which(stats_N$natural_death == 1)
unnatural <- which(stats_N$unnatural_death == 1)
# Are there any people with ambiguous causes of death? -- No!
length(intersect(natural, unnatural))

cluster_feats <- c('mean_before', 'peak_mean_boost', 'perm_boost', 'time_till_half')
cluster_result_N <- cluster(stats_N, cluster_feats)
cluster_result_T <- cluster(stats_T, cluster_feats)
k <- 4
km_N <- cluster_result_N$clustering[[which(cluster_result_N$K==k)]]
km_T <- cluster_result_T$clustering[[which(cluster_result_T$K==k)]]
```

# Number of documents per day

```{r plot num art, fig.width=10, fig.height=4, echo=if (!SUPP_INFO) -c(1,4) else FALSE}
par(mfrow=c(1,2))
plot_num_art(num_art_N, 'NEWS')
plot_num_art(num_art_T, 'TWITTER')
par(.default_par)
```

`r figcap(TRUE)`
Number of documents per day in the Spinn3r corpus.

\ifshowall
## Average daily number of documents by year

News:

```{r}
tapply(num_art_N, substring(names(num_art_N),0,4), function(x) mean(x, na.rm=TRUE))
```

Twitter:

```{r}
tapply(num_art_T, substring(names(num_art_T),0,4), function(x) mean(x, na.rm=TRUE))
```
\fi


# Taxonomy of causes of death

## Natural causes of death

`r sprintf(paste(sort(c(as.character(tax_causes$cause.of.death[tax_causes$level1=='natural']), strsplit(natural_manual, '\\|')[[1]])), collapse=', '))`

## Unnatural causes of death

`r sprintf(paste(sort(c(as.character(tax_causes$cause.of.death[tax_causes$level1=='unnatural']), strsplit(unnatural_manual, '\\|')[[1]])), collapse=', '))`

`r #paste(sort(c('', as.character(tax_causes$cause.of.death[tax_causes$level1=='unnatural']), strsplit(unnatural_manual, '\\|')[[1]])), collapse='\n* ')`

# Taxonomy of notability types

```{r, echo=FALSE}
tt <- tax_types[tax_types$profession != '[blank]',]
type2str <- tapply(tt$profession, tt$level1, function(s) paste(sort(s), collapse=', '))
```

`r sprintf('* %s', paste(sprintf('__%s:__', names(type2str)), type2str, collapse='\n* '))`


# Biographic statistics of public figures

\begin{center}
\begin{tabular}{lrrr}
  \hline
 & \textbf{All} & \textbf{Included} & \textbf{Regression} \\ 
  \hline
\textbf{Age} &  &  &  \\ 
  \hspace{5mm} N/A & 10\% & 6\% & 0\% \\ 
  \hspace{5mm} 1st quartile & 68 & 64 & 60 \\ 
  \hspace{5mm} Mean & 76 & 74 & 70 \\ 
  \hspace{5mm} Median & 80 & 77 & 73 \\ 
  \hspace{5mm} 3rd quartile & 88 & 87 & 84 \\ 
   \hline
\textbf{Gender} &  &  &  \\ 
  \hspace{5mm} N/A & 27\% & 7\% & 0\% \\ 
  \hspace{5mm} Female & 16\% & 17\% & 17\% \\ 
  \hspace{5mm} Male & 84\% & 83\% & 83\% \\ 
   \hline
\textbf{Manner of death} &  &  &  \\ 
  \hspace{5mm} N/A & 76\% & 60\% & 0\% \\ 
  \hspace{5mm} Natural & 85\% & 86\% & 88\% \\ 
  \hspace{5mm} Unnatural & 15\% & 14\% & 12\% \\ 
   \hline
\textbf{Language} &  &  &  \\ 
  \hspace{5mm} N/A & 45\% & 27\% & 14\% \\ 
  \hspace{5mm} Anglophone & 60\% & 82\% & 80\% \\ 
  \hspace{5mm} Non-anglophone & 40\% & 18\% & 20\% \\ 
   \hline
\textbf{Notability type} &  &  &  \\ 
  \hspace{5mm} N/A & 1\% & 0\% & 0\% \\ 
  \hspace{5mm} Arts & 40\% & 50\% & 56\% \\ 
  \hspace{5mm} Sports & 14\% & 14\% & 14\% \\ 
  \hspace{5mm} Leadership & 11\% & 14\% & 13\% \\ 
  \hspace{5mm} Known for death & 26\% & 16\% & 10\% \\ 
  \hspace{5mm} General fame & 7\% & 4\% & 5\% \\ 
  \hspace{5mm} Academia/engineering & 2\% & 2\% & 2\% \\ 
   \hline
\textbf{Count} & 33\,340 & 2\,362 & 870 \\ 
   \hline
\end{tabular}
\end{center}

\ifshowall
```{r people description 1, include=!SUPP_INFO}
table <- make_bio_table()
table
```
\fi
`r tabcap(TRUE)`
__Biographic statistics of public figures.__
_All_ refers to the 33\,340 individuals listed in the Freebase knowledge base as having died between 11 June 2009 and 30 September 2014.
_Included_ refers to the 2\,362 individuals included in the study (exclusion was mostly due to below-threshold pre-mortem mention frequencies, cf. _Materials and Methods_ in the main paper).
_Regression_ is a subset of _Included_ and refers to the 870 individuals included in the regression analysis (exclusion was due to unknown age, gender, or manner of death).
_N/A_ refers to the percentage of individuals for whom the respective property was not available in Freebase.
The remaining relative frequencies were computed based on the set of individuals for whom the property was available, so they sum to 100\%.

```{r people description 2, echo=FALSE}
print(xtable(table, caption='', label='tab:people stats', align='lrrr'),
      hline.after=c(-1,0,6,10,14,18,nrow(table)-1,nrow(table)),
      file=sprintf('%s/people_stats_table.tex', TABLEDIR), floating=FALSE)
```

\ifshowall
# Examples of individual mention time series

`r '__(a) News__'`

```{r plot example time series N, fig.width=10, fig.height=2.5, echo=if (!SUPP_INFO) -1 else FALSE}
par(mfrow=c(1,4))
plot_time_series_for_small_multiples('Benoit_Mandelbrot', 'NEWS', X_N, x_N, num_art_N, ylab=TRUE)
plot_time_series_for_small_multiples('Kashiram_Rana', 'NEWS', X_N, x_N, num_art_N, ylab=FALSE)
plot_time_series_for_small_multiples('Amy_Winehouse', 'NEWS', X_N, x_N, num_art_N, ylab=FALSE)
plot_time_series_for_small_multiples('George_McGovern', 'NEWS', X_N, x_N, num_art_N, ylab=FALSE)
```

`r '__(b) Twitter__'`

```{r plot example time series T, fig.width=10, fig.height=2.5, echo=if (!SUPP_INFO) -c(1,6) else FALSE}
par(mfrow=c(1,4))
plot_time_series_for_small_multiples('Benoit_Mandelbrot', 'TWITTER', X_T, x_T, num_art_T, ylab=TRUE)
plot_time_series_for_small_multiples('Kashiram_Rana', 'TWITTER', X_T, x_T, num_art_T, ylab=FALSE)
plot_time_series_for_small_multiples('Amy_Winehouse', 'TWITTER', X_T, x_T, num_art_T, ylab=FALSE)
plot_time_series_for_small_multiples('George_McGovern', 'TWITTER', X_T, x_T, num_art_T, ylab=FALSE)
par(.default_par)
```

`r figcap(!SUPP_INFO)`
Examples of mention time series for four deceased public figures.
\fi


\ifshowall
# Overlay of all mention time series

```{r scary wire plot, fig.width=8, fig.height=4, echo=if (!SUPP_INFO) -c(1,4) else FALSE}
par(mfrow=c(1,2))
overlay_time_series(X_N, 'NEWS')
overlay_time_series(X_T, 'TWITTER')
par(.default_par)
```

`r figcap(!SUPP_INFO)`
Overlay of all mention time series.
\fi


\ifshowall
# Frequency of post-mortem peak days

## Histogram of post-mortem peak days

```{r max mem hist, fig.width=10, fig.height=3, echo=if (!SUPP_INFO) -c(1,4) else FALSE}
par(mfrow=c(1,2))
mmh_N <- plot_max_mem_hist('NEWS')
mmh_T <- plot_max_mem_hist('TWITTER')
par(.default_par)
```

`r figcap(!SUPP_INFO)`
Histogram of post-mortem peak days. _Left:_ news. _Right:_ Twitter. For each post-mortem day we count for how many individuals the mention frequency is largest on that day, out of the 100 days immediately following death. For most people, the maximum mention frequency is observed on the day after death.

## Relative frequencies of peak days

The first entry of each vector corresponds to day 0 (i.e., the day of death).

```{r, echo=!SUPP_INFO}
h_N <- mmh_N$hist
h_T <- mmh_T$hist
max_days_N <- mmh_N$max_days
max_days_T <- mmh_T$max_days

probs_N <- (h_N$counts/sum(h_N$counts))[1:30]
probs_T <- (h_T$counts/sum(h_T$counts))[1:30]
```

__News:__

```{r, echo=!SUPP_INFO}
probs_N
```

__Twitter:__

```{r, echo=!SUPP_INFO}
probs_T
```

## Cumulative relative frequencies of peak days

The first entry of each vector corresponds to day 0 (i.e., the day of death).

__News:__

```{r, echo=!SUPP_INFO}
cumsum(probs_N)
```

__Twitter:__

```{r, echo=!SUPP_INFO}
cumsum(probs_T)
```

## People with late peak days (after day 29)

Those whose peak day comes after day 29 tend to be those with low short-term boosts, as seen by computing the relative rank with respect to short-term boost of those with a late peak.

__News:__

```{r, echo=!SUPP_INFO}
max_rank <- length(max_days_N)-1
(summary(stats_N$peak_mean_boost_rank[max_days_N>29])-1) / max_rank
# Compare this to the fraction of people with a late peak day.
(sum(max_days_N>29)-1) / max_rank
```

__Twitter:__

```{r, echo=!SUPP_INFO}
(summary(stats_T$peak_mean_boost_rank[max_days_T>29])-1) / max_rank
# Compare this to the fraction of people with a late peak day.
(sum(max_days_T>29)-1) / max_rank
```
\fi


\ifshowall
# Average mention time series

```{r, echo=!SUPP_INFO}
bipower_model_N <- bipower_fit('NEWS')
bipower_model_T <- bipower_fit('TWITTER')
```

`r '__(a) News__'`

```{r, fig.width=6, fig.height=5, echo=!SUPP_INFO}
plot_avg_fraction_of_mentioning_docs('NEWS', bipower_model_N, 'Bipower')
```

`r '__(b) Twitter__'`

```{r, fig.width=6, fig.height=5, echo=!SUPP_INFO}
plot_avg_fraction_of_mentioning_docs('TWITTER', bipower_model_T, 'Bipower')
```

`r figcap(!SUPP_INFO)`
Average mention time series, obtained via the arithmetic mean of the individual raw mention time series of the people included in the study,
for __(a)__ news and __(b)__ Twitter.


## Bipower model parameters

News:

```{r, echo=!SUPP_INFO}
coef(bipower_model_N)
```

Twitter:

```{r, echo=!SUPP_INFO}
coef(bipower_model_T)
```
\fi


# Model comparison

__(a) News__
```{r alternative_models_N, fig.width=3.5, fig.height=10, echo=!SUPP_INFO}
plot_all_model_fits('NEWS')
```

__(b) Twitter__
```{r alternative_models_T, fig.width=3.5, fig.height=10, echo=!SUPP_INFO, warning=!SUPP_INFO}
plot_all_model_fits('TWITTER')
```

`r figcap(TRUE)`
Comparison of nine models $S(t)$ for fitting empirical mention frequencies, for __(a)__ the news and __(b)__ Twitter.
All $y$-axes are logarithmic.
The left and right plots in each row show the same fits, the only difference being that the left plots have linear $x$-axes, whereas the right plots have logarithmic $x$-axes.
Fits are nonlinear least-squares fits obtained in log space, i.e., logarithms were taken of both the empirical data and the model $S(t)$ before performing the least-squares optimization (see equation (6) in the paper for the case of the bipower model).
The bipower function is defined in equation (4) of the paper.
The biexponential function was introduced by Candia et al. ("The universal decay of collective memory and attention." _Nature Human Behaviour_. 2019; 3(1):82-91) and is parameterized by $N,p,q,r>0$, as follows:

* Biexponential: $S(t) = \frac{N}{p+r-q} \left(  (p-q) e^{-(p+r)t} + re^{-qt} \right)$.

Theoretical motivations for six of the seven remaining functions are given by Rubin and Wenzel ("One hundred years of forgetting: A quantitative description of retention." _Psychological Review_. 1996; 103(4):734-760).
Four of these functions are parameterized by two parameters $a,b>0$, as follows:

* Exponential: $S(t) = a \, e^{-bt}$, i.e., $\log S(t) = \log a - b e^{\log t}$.
* Hyperbolic: $S(t) = (a + bt)^{-1}$, i.e., $\log S(t) = -\log(a + b e^{\log t})$.
* Logarithmic: $S(t) = a - b \log t$, i.e., $\log S(t) = \log(a - b \log t)$.
* Power: $S(t) = a \, t^{-b}$, i.e., $\log S(t) = \log a - b \log t$.

The four above functions share the property of being concave (exponential, hyperbolic, logarithmic) or linear (power) when plotted on log-log axes (i.e., $\log S(t)$ is a concave function of $\log t$, cf. right columns), whereas the empirical curves are convex on log-log axes.

Rubin and Wenzel also proposed generalized versions of the exponential and hyperbolic functions, called exponential-power and hyperbolic-power functions, respectively, where $t$ is replaced by the power $t^c$, where $c$ is a third parameter.
(Analogous generalized versions of the logarithmic and power functions are not necessary, as they can already be expressed by the plain logarithmic and power functions, since $b \log t^c = (bc) \log t$.)
For $c>0$, the exponential-power and hyperbolic-power functions, too, are concave in log-log space, but when allowing for $b,c < 0$, they can be made convex and are thus better suited for fitting the empirical data (note that, in the following specifications, we maintain $a,b,c > 0$, but replace $b$ by $-b$, and $c$ by $-c$):

* Exponential-power: $S(t) = a \, e^{bt^{-c}}$, i.e., $\log S(t) = \log a + b e^{-c \log t}$.
* Hyperbolic-power: $S(t) = (a - bt^{-c})^{-1}$, i.e., $\log S(t) = -\log (a - be^{-c \log t})$.

Finally, as the last function, we consider what Candia et al. refer to as the "log-normal" function, defined as
$S(t) = \exp(\log a - b \log t - c (\log t)^2)$.
To recognize the fact that, although this function takes the functional form of the log-normal distribution, it is not actually used to describe a probability distribution here, we refer to the function as "log-normal-based".
The log-normal-based function, too, is concave in log-log space, but can be made convex by replacing $c$ by $-c$, i.e.,
$S(t) = \exp(\log a - b \log t + c (\log t)^2)$.
Note, however, that this results in $S(t)$ being an increasing function of $t$ as $t \rightarrow \infty$, unlike the empirical data and unlike what one would require from a sound theoretical model of collective memory.
We hence constrain the parameters such that the fitted function is monotonically decreasing over the modeled time range (days 1 to $t_{\max}=400$).
Since the unconstrained function, when fitted to the empirical data, assumes a minimum at $1<t<t_{\max}$, the monotonicity constraint is equivalent to requiring the minimum to occur at $t=t_{\max}$, which happens for $b=-2ct_{\max}$ and gives rise to the following model:

* Log-normal-based: $S(t) = \exp(\log a + c ((\log t)^2 - 2t_{\max}\log t))$, i.e., $\log S(t) = \log a + c ((\log t)^2 - 2t_{\max}\log t)$.

We quantify goodness of fit using two measures (results in the legends of the right plots):
(1) via coefficients of determination ($R^2$; computed as the squared correlation between observed and predicted values on the log scale; larger is better);
(2) in order to account for the varying model complexity (the models have between two and four parameters), via Akaike's information criterion (AIC; smaller is better).
$R^2$ and AIC result in the same ordering of the eight models, and the ordering is identical across the two media (news and Twitter).
In the figure, the models are sorted, top-down, in increasing order of goodness of fit.
The bipower model provides the best fit according to both measures ($R^2$ and AIC) and for both media (news and Twitter), with $R^2=0.989$ for the news and $R^2=0.985$ for Twitter.


# Proportion of cultural vs. communicative memory

```{r plot_comm_vs_cult_mem, fig.width=9, fig.height=3, echo=if (!SUPP_INFO) -c(1,4) else FALSE}
par(mfrow=c(1,2))
plot_comm_vs_cult_mem('NEWS', coef(bipower_model_N), 'Bipower')
plot_comm_vs_cult_mem('TWITTER', coef(bipower_model_T), 'Bipower')
par(.default_par)
```

`r figcap(TRUE)`
Proportion of cultural vs. communicative memory.
On day $t$ after death, the total collective memory according to the bipower model fit is $S(t)=u(t)+v(t)$, where $u(t)$ is the communicative memory, and $v(t)$ is the cultural memory.
We plot the proportion $v(t)/S(t)$ as a function of $t$.
_Left:_ news. _Right:_ Twitter.


# Document length

```{r doc length, fig.width=10, fig.height=4, echo=if (!SUPP_INFO) -c(1) else FALSE}
par(mfrow=c(1,2))
regdata <- load_doc_length_regression_data()
# Only constant, i.e., estimate mean (plus standard errors).
plot_doc_length_increase(regdata, '1', 'Direct estimate')
# Adjust for those predictors that were found to have a significant impact on short-term
# post-mortem mention frequency in the news.
# (All possible predictors: mean_before_relrank, age_group, death_type, gender, anglo, type_group.)
plot_doc_length_increase(regdata, 'mean_before_relrank + age_group + death_type + anglo', 'Adjusted estimate')

```
`r figcap(TRUE)`
Relative length increase of news documents that mention deceased public figures, with respect to the respective public figure's pre-mortem mean document length, as a function of days since death.
All means in this analysis are geometric means.
Error bars are 95% confidence intervals approximated as $\pm 2$ standard errors.
__Left:__ Direct mean estimates of relative length increase.
__Right:__ Estimates adjusted for population drift (since certain groups of people are more likely to be mentioned post-mortem than others). Each day $t$'s adjusted estimate was obtained from a separate linear regression model for day $t$ only, which included each person $i$ mentioned that day as a data point, with $\log(L_{it}/P_i)$ as the dependent variable (where $L_{it}$ is $i$'s mean document length on day $t$, and $P_i$ is $i$'s pre-mortem mean document length),
and with independent variables defined by the predictors that were previously found to be significantly associated with short-term post-mortem mention frequency (language, pre-mortem mean mention frequency rank, manner of death, age group), such that estimates are for anglophones of median pre-mortem popularity who died an unnatural death at age 70--79 years.
The adjusted estimate of the (geometric) mean relative length increase for day $t$ is then given by $e^{a_t}-1$, where $a_t$ is the intercept of the regression for day $t$.
We see that, both with and without adjustment, the documents that mentioned a person on the day of their death were on average about 40% shorter than documents that mentioned the person before their death, and that the pre-mortem level is reached again after about one month.


# Mention time series characteristics

## Summaries of distributions of mention time series characteristics

__(a) News__
```{r curve char summ N, echo=!SUPP_INFO}
summ <- summary(stats_N[, c('mean_before', 'peak_mean_boost', 'perm_boost', 'time_till_half')])
colnames(summ) <- FANCY_CURVE_CHAR_NAMES[trimws(colnames(summ))]
summ
```
__(b) Twitter__
```{r curve char summ T, echo=!SUPP_INFO}
summ <- summary(stats_T[, c('mean_before', 'peak_mean_boost', 'perm_boost', 'time_till_half')])
colnames(summ) <- FANCY_CURVE_CHAR_NAMES[trimws(colnames(summ))]
summ
```
`r tabcap(TRUE)`
Summaries of distribution of mention time series characteristics for __(a)__ the news and __(b)__ Twitter.

`r vertskip`

```{r curve char distribution plots N, fig.width=10, fig.height=2.5, echo=!SUPP_INFO}
plot_smoothed_densities_raw(stats_N, 'NEWS', 'mid', list(unique(stats_N$mid)), main='NEWS')
plot_smoothed_densities_raw(stats_T, 'TWITTER', 'mid', list(unique(stats_T$mid)), main='TWITTER')
```
`r figcap(TRUE)`
Kernel-smoothed density plots of mention time series characteristics for the news (top) and Twitter (bottom).


\ifshowall
## Magnitude of short- and long-term boosts

### Short-term boost

News:

```{r, echo=!SUPP_INFO}
boot_ci(stats_N$peak_mean_boost, median)
wilcox.test(stats_N$peak_mean_boost, mu=0, conf.int=TRUE)
```

Twitter:

```{r, echo=!SUPP_INFO}
boot_ci(stats_T$peak_mean_boost, median)
wilcox.test(stats_T$peak_mean_boost, mu=0, conf.int=TRUE)
```

### Comparison of short-term boosts in news vs. Twitter

```{r, echo=!SUPP_INFO}
wilcox.test(x=stats_N$peak_mean_boost, y=stats_T$peak_mean_boost, paired=TRUE, conf.int=TRUE)
```

### Long-term boost

News:

```{r, echo=!SUPP_INFO}
boot_ci(stats_N$perm_boost, median)
wilcox.test(stats_N$perm_boost, mu=0, conf.int=TRUE)
```

Twitter:

```{r, echo=!SUPP_INFO}
boot_ci(stats_T$perm_boost, median)
wilcox.test(stats_T$perm_boost, mu=0, conf.int=TRUE)
```

### Comparison of long-term boosts in news vs. Twitter

```{r, echo=!SUPP_INFO}
wilcox.test(x=stats_N$perm_boost, y=stats_T$perm_boost, paired=TRUE, conf.int=TRUE)
```
\fi


## Mention time series characteristics by notability type

__(a) News__

```{r curve char by type group N, fig.width=10, fig.height=4, echo=!SUPP_INFO, warning=FALSE}
plot_chars_by_group(stats_N, 'NEWS', stats_N$type_group, type_groups)
```

__(b) Twitter__

```{r curve char by type group T, fig.width=10, fig.height=4, echo=!SUPP_INFO, warning=FALSE}
plot_chars_by_group(stats_T, 'TWITTER', stats_T$type_group, type_groups)
```
`r figcap(TRUE)`
Distributions of mention time series characteristics by notability type, visualized as box plots, for __(a)__ the news and __(b)__ Twitter.
Boxes are bounded by the first and third quartiles; whiskers extend 1.5 inter-quartile ranges beyond the first and third quartiles (or to the minimum/maximum in case they fall within 1.5 inter-quartile ranges); the center bars mark medians, with notches corresponding to 95% confidence intervals of the median.



# Cluster analysis

## Optimal number of clusters

```{r compute clusters, fig.width=10, fig.height=4, echo=if (!SUPP_INFO) -c(1,4) else FALSE}
par(mfrow=c(1,2))
plot_sil_width('NEWS', cluster_result_N$clustering, cluster_result_N$diss)
plot_sil_width('TWITTER', cluster_result_T$clustering, cluster_result_T$diss)
par(.default_par)
```
`r figcap(TRUE)`
Average silhouette width of clusterings produced by $k$-means algorithm, as a function of the number $k$ of clusters (higher is better), for $k \in \{2, \dots, 30\}$.
Both __(a)__ in the news and __(b)__ on Twitter, $k=4$ is optimal.


## Overlay of time series per cluster

```{r plot clustered time series, fig.width=10, fig.height=2.5, echo=!SUPP_INFO}
plot_clustered_time_series('NEWS', km_N, X_N)
plot_clustered_time_series('TWITTER', km_T, X_T)
```
`r figcap(TRUE)`
Overlay of time series in each cluster, for news (top) and Twitter (bottom).


\ifshowall
## Analysis of cluster confusion matrix

### News and Twitter are not independent

```{r echo=!SUPP_INFO}
X <- table(as.factor(km_N$cluster), as.factor(km_T$cluster))
chisq.test(X, simulate.p.value=TRUE, B=1e5)
```

### Log ratios of empirical vs. null probabilities

Null probabilites represent the case where news and Twitter are assumed to be independent (with the constraint that cluster sizes must remain constant).

The following matrix shows the log10 ratio of the empirical cluster size and the cluster size under the null model (rows are news cluster, columns are Twitter clusters).
That is, positive values imply overrepresentation in the empirical data, and negative values, underrepresentation.
We see that all diagonal entries are overrepresented, whereas all off-diagonal entries except two---$(3,4)$ and $(4,3)$---are underrepresented.

```{r echo=!SUPP_INFO}
X.indep <- (rowSums(X) %o% colSums(X)) / sum(X)
log10(X / X.indep)
```

### Overrepresentation of diagonal entries

To quantify the overrepresentation of the diagonal, we compute four diagonal sums (everything under the constraint that the row and column sums are fixed to the empirical values):

1. the minimum possible,
2. the independent null model,
3. the empirical data,
4. the maximum possible.

The minimum and maximum are computed via linear programming.

```{r echo=!SUPP_INFO}
k <- nrow(X)
objective.in <- as.vector(diag(nrow=k))
const.mat <- rbind(t(diag(1,k) %x% rep(1,k)),
                   t(rep(1,k) %x% diag(1,k)))
const.rhs <- c(colSums(X), rowSums(X))
const.dir <- rep('==', k)
opt.max <- lp('max', objective.in, const.mat, const.dir, const.rhs)
X.max <- matrix(opt.max$solution, k, k)
opt.min <- lp('min', objective.in, const.mat, const.dir, const.rhs)
X.min <- matrix(opt.min$solution, k, k)
```

The values of the four above-described diagonal sums are as follows:

```{r echo=!SUPP_INFO}
sums <- c(sum(diag(X.min)), sum(diag(X.indep)), sum(diag(X)), sum(diag(X.max)))
sums
```

Normalizing by the min-to-max span, we see that the empirical diagonal gets 71% of the way between min and max, whereas the null only gets 32%, for a factor of 2.2.

```{r echo=!SUPP_INFO}
(sums - sum(diag(X.min))) / (sum(diag(X.max)) - sum(diag(X.min)))
```

### Overrepresentation of cluster combinations (3, 4) and (4, 3)

On the contrary, nearly all off-diagonal entries are underrepresented, with the exception of $(4,3)$, which is not significantly different from the null:

```{r echo=!SUPP_INFO}
prop.test(X[4,3], sum(X), X.indep[4,3]/sum(X))
```

and $(3,4)$, which is massively overrepresented, compared to the null:

```{r echo=!SUPP_INFO}
prop.test(X[3,4], sum(X), X.indep[3,4]/sum(X))
```
\fi



# Regression modeling

Note: the variable names in the code may differ from those in the paper; see mapping in lists below.
 
Independent variables:

* pre-mortem mean (`mean_before`)
* age at death (`age_group`)
* manner of death (`death_type`)
* notability type (`type_group`)
* language (`anglo`)
* gender (`gender`)

Dependendent variables:

* short-term boost (`peak_mean_boost`)
* long-term boost (`perm_boost`)

We report results for two variants of each model, which differ in the way dependent variables are treated. Model variants are marked via the "Transformation on dependent variable" descriptor in the header of each model. The two variants are the following:

* `NONE`: dependent variables were used as-is. This variant is used in the main paper.
* `RELATIVE RANKS`: dependent variables were transformed to relative ranks; i.e., they were rank-transformed and then shifted/scaled to the interval $[-0.5,0.5]$. This way, dependent variables need to be interpreted in relative terms: a value of 0 corresponds to the median, and positive [negative] values to ranks above [below] the median.

## Models without interactions

```{r run regressions, echo=!SUPP_INFO}
for (ranks in c(FALSE, TRUE)) {
  mods_N <- run_regression_analysis('NEWS', ranks)
  mods_T <- run_regression_analysis('TWITTER', ranks)
  
  print_regression_table(mods_N, mods_T, ranks)
  
  print_model_description('NEWS', 'short-term boost', ranks)
  print(summary(mods_N$mod_peak))
  
  print_model_description('NEWS', 'long-term boost', ranks)
  print(summary(mods_N$mod_perm))
  
  print_model_description('TWITTER', 'short-term boost', ranks)
  print(summary(mods_T$mod_peak))
  
  print_model_description('TWITTER', 'long-term boost', ranks)
  print(summary(mods_T$mod_perm))
}
```

## Models with "age by manner of death" interaction

```{r run regression models with interactions, fig.width=10, fig.height=3, echo=if (!SUPP_INFO) -1 else FALSE, fig.keep=if (SUPP_INFO) "none" else "high"}
for (ranks in c(FALSE, TRUE)) {
  for (medium in c('NEWS', 'TWITTER')) {
    mods <- run_regression_analysis_for_death_types(medium, ranks)
    # Short-term.
    print_model_description(medium, 'short-term boost', ranks)
    print(summary(mods$mod_peak))
    # Long-term.
    print_model_description(medium, 'long-term boost', ranks)
    print(summary(mods$mod_perm))
    # Plots.
    par(mfrow=c(1,2))
    plot_age_coeffs_for_death_types(mods$mod_peak, medium, 'peak_mean_boost', BASE_AGE, ranks,
                                    if (ranks) c(-0.3,0.6) else c(1.8,4.2))
    plot_age_coeffs_for_death_types(mods$mod_perm, medium, 'perm_boost', BASE_AGE, ranks,
                                    if (ranks) c(-0.3,0.6) else c(-0.1,0.6))
  }
}
```

## Models of news-vs.-Twitter boost difference for fixed person (without interactions)

```{r regression analysis NEWS vs. TWITTER, echo=!SUPP_INFO}
for (ranks in c(FALSE, TRUE)) {
  mods <- run_regression_analysis_T_vs_N(ranks)
  
  print_regression_table_T_vs_N(mods, ranks)
  
  print_model_description('NEWS vs. TWITTER', 'short-term boost', ranks)
  print(summary(mods$mod_peak))
  
  print_model_description('NEWS vs. TWITTER', 'long-term boost', ranks)
  print(summary(mods$mod_perm))
}
```


## Models of news-vs.-Twitter boost difference for fixed person (with "age by manner of death" interaction)

```{r regression N vs T, fig.width=10, fig.height=3, echo=!SUPP_INFO, fig.keep=if (SUPP_INFO) "none" else "high"}
for (ranks in c(FALSE, TRUE)) {
  mods <- run_regression_analysis_T_vs_N_for_death_types(ranks)
  # Short-term.
  print_model_description('NEWS vs. TWITTER', 'short-term boost', ranks)
  print(summary(mods$mod_peak))
  # Long-term.
  print_model_description('NEWS vs. TWITTER', 'long-term boost', ranks)
  print(summary(mods$mod_perm))
  # Plots.
  par(mfrow=c(1,2))
  plot_age_coeffs_for_death_types(mods$mod_peak, NA, 'peak_mean_boost_diff', BASE_AGE, ranks,
                                  if (ranks) c(-0.45, 0.45) else c(-1.5, 0.5), 'black')
  plot_age_coeffs_for_death_types(mods$mod_perm, NA, 'perm_boost_diff', BASE_AGE, ranks,
                                  if (ranks) c(-0.45, 0.45) else c(-0.5, 0.5), 'black')
}
```


```{r mega figure, fig.width=10, fig.height=10, include=FALSE}
draw_mega_figure()
```

# Comprehensiveness of Spinn3r

Although Spinn3r does not publicly disclose its data collection strategy, we confirm the comprehensiveness of the corpus as follows.
The news portion of the Spinn3r corpus contains documents from each of the 6\,608 English-language news domains identified externally via Google News, which hints at Spinn3r's completeness with respect to the entirety of online news.
Moreover, in each of the four years (2010--2013) that are covered in their entirety by our study period, the average daily number of English tweets in the Spinn3r corpus amounts to 8--19\% of Twitter's full tweet volume across all languages.$^1$
With English tweets estimated to have accounted for 53\% of all tweets in 2010$^2$ and 34\% in 2013$^3$, we estimate that the Spinn3r corpus contains about one third of all English tweets, as summarized in the following table:

\begin{center}
\begin{tabular}{r|r|r|r|r}
        & Full Twitter     & Spinn3r           & Full Twitter    & Completeness\\
Year    & All tweets$^1$   & English tweets    & English tweets$^{2,3}$ & English tweets\\
\hline
2010 & 35 million/day     &  6.7 million/day & 19 million/day & 36\% \\
2011 & 200 million/day     &  20 million/day &  &  \\
2012 & 340 million/day     &  28 million/day &  &  \\
2013 & 500 million/day     &  51 million/day & 170 million/day  & 30\% \\
\hline
\end{tabular}
\end{center}

\ifshowall
<table border="1">
<tr><td>Year    </td><td> Full Twitter<br/>All tweets$^1$   </td><td> Spinn3r<br/>English tweets    </td><td> Full Twitter<br/>English tweets$^{2,3}$ </td><td> Completeness<br/>English tweets</td></tr>
<tr><td>2010 </td><td> 35 million/day     </td><td>  6.7 million/day </td><td> 19 million/day </td><td> 36% </td></tr>
<tr><td>2011 </td><td> 200 million/day     </td><td>  20 million/day </td><td>  </td><td>  </td></tr>
<tr><td>2012 </td><td> 340 million/day     </td><td>  28 million/day </td><td>  </td><td>  </td></tr>
<tr><td>2013 </td><td> 500 million/day     </td><td>  51 million/day </td><td> 170 million/day  </td><td> 30% </td></tr>
</table>
<br/>
\fi
`r tabcap(TRUE)`
Completeness estimation of the English-language portion of the Spinn3r corpus, with respect to the entirety of English-language tweets on full Twitter.

$^1$ https://www.internetlivestats.com/twitter-statistics/

$^2$ B. Poblete, R. Garcia, M. Mendoza, A. Jaimes. Do all birds tweet the same? Characterizing Twitter around the world. In _Proceedings of the 20th ACM International Conference on Information and Knowledge Management_. pp. 1025--1030 (2011).

$^3$ http://www.statista.com/statistics/267129/most-used-languages-on-twitter/
